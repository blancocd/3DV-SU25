{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27c9b995",
   "metadata": {},
   "source": [
    "# Epipolar Geometry and Eight Point Algorithm (20 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6f980",
   "metadata": {},
   "source": [
    "## Epipolar Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyvista as pv\n",
    "import trimesh\n",
    "from utils.utils import (\n",
    "    load_scene_params,\n",
    "    load_scene_params2,\n",
    "    plot_epipolar_setup,\n",
    "    draw_dot,\n",
    "    draw_line,\n",
    "    superglue_correspondences,\n",
    "    draw_matches,\n",
    "    draw_candidates,\n",
    "    draw_pair_with_reconstruction,\n",
    "    test_normalize_points\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bedecdd",
   "metadata": {},
   "source": [
    "We have prepared a couple of scenes containing some object which is viewed from two different angles. Load scene parameters, 3D mesh and images using `load_scene_params`. `scene_params` contains the camera intrinsic and extrinsic parameters, as well as the projection matrices and global positions of the camera centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c329112",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_params, mesh, img0, img1 = load_scene_params(scene_id=\"01\")\n",
    "print(scene_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the two camera views side by side\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot first image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img0)\n",
    "plt.title(\"Camera 0\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot second image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Camera 1\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce55c8",
   "metadata": {},
   "source": [
    "You can also visualize the setting in 3D using `plot_epipolar_setup`. You should get an interactive 3D plot of the scene with the camera centers, camera frustrums, camera coordinate systems, and 3D mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = plot_epipolar_setup(mesh, img0, img1, scene_params)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673908d",
   "metadata": {},
   "source": [
    "# Epipolar Geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397f6bd",
   "metadata": {},
   "source": [
    "### Essential Matrix and Fundamental Matrix Computation (2 Points)\n",
    "Implement the function `compute_E_F` that given the scene params, computes the essential matrix $E$ and fundamental matrix $F$ from the camera intrinsics and the relative camera pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_E_F(scene_params):\n",
    "    \"\"\"\n",
    "    Takes as input the scene parameters dictionary and returns (E, F) the essential and fundamental matrix.\n",
    "    \"\"\"\n",
    "    # TODO: Implement your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b54dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "E, F = compute_E_F(scene_params)\n",
    "print(\"Essential Matrix:\\n\", E)\n",
    "print(\"Fundamental Matrix:\\n\", F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c907378",
   "metadata": {},
   "source": [
    "Verify the epipolar constraint by taking a random 3D mesh point, computing the projections to the image planes and evaluating $u_0^T F u_1$. The results should be close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_ids = np.random.randint(0, len(mesh.vertices), 10)\n",
    "p3d = mesh.vertices[point_ids]\n",
    "\n",
    "## Verify the epipolar constraint using the points above.\n",
    "## TODO: Implement your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1b5e83",
   "metadata": {},
   "source": [
    "### Epipoles (1 Points)\n",
    "Recall that the epipoles are the image points where the baseline intersects the image planes. In other words, the epipoles are the projections of the camera centers in the other image. From the lecture you know that the epipoles can be computed from the fundamental matrix. Find the epipoles in both images and plot them on the images using the function `draw_dot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the epipoles e0 and e1 in image coordinates.\n",
    "## TODO: Implement your code here\n",
    "e0 = ...\n",
    "e1 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b5350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw epipoles in green in images.\n",
    "img0_ep = img0.copy()\n",
    "img1_ep = img1.copy()\n",
    "# Draw epipoles in images\n",
    "img0_ep = draw_dot(img0_ep, e0, color=(0, 255, 0, 255), radius=7)\n",
    "img1_ep = draw_dot(img1_ep, e1, color=(0, 255, 0, 255), radius=7)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img0_ep)\n",
    "plt.title(\"Epipole in image 0\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img1_ep)\n",
    "plt.title(\"Epipole in image 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea3f51",
   "metadata": {},
   "source": [
    "### Epipolar Lines (2 Points)\n",
    "\n",
    "Given a 3D point, we want to find the epipolar lines in the two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b198b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PID = 100 # Point ID. Try: 354 (nose), 21926 (medal), 20405 (hair)\n",
    "point = mesh.vertices[PID]\n",
    "\n",
    "# Find the epipolar lines in both images for the 3D point point.\n",
    "# TODO: Implement your code here\n",
    "l0 = ...\n",
    "l1 = ...\n",
    "\n",
    "# Draw epipolar lines in images\n",
    "img0_line = img0.copy()\n",
    "img1_line = img1.copy()\n",
    "img0_line = draw_line(img0_line, l0, color=(255, 0, 0, 255))\n",
    "img1_line = draw_line(img1_line, l1, color=(255, 0, 0, 255))\n",
    "img0_line = draw_dot(img0_line, e0, color=(0, 255, 0, 255), radius=7)\n",
    "img1_line = draw_dot(img1_line, e1, color=(0, 255, 0, 255), radius=7)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img0_line)\n",
    "plt.title(\"Epipolar line in image 0\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img1_line)\n",
    "plt.title(\"Epipolar line in image 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97037cb",
   "metadata": {},
   "source": [
    "# Estimating Relative Pose of Two Cameras From 2D-2D Correspondences\n",
    "\n",
    "In this exercise we will implement the normalized 8-point algorithm to estimate the fundamental matrix from 2D-2D point correspondences. Then we will decompose the essential matrix to estimate the relative pose of the two cameras. Finally, we will triangulate the 3D points from the 2D-2D correspondences and the estimated relative pose.\n",
    "\n",
    "## Correspondences\n",
    "We will use SuperPoint and SuperGlue for feature extraction and matching. Note that these correspondence matching algorithms are not perfect and will produce outliers. So if you observe inaccurate results, it can be due to bad correspondences. Below we provide a quick and dirty way of computing perfect correspondences via projection. We advise you to use this for debugging your code, to eliminate the effect of bad correspondences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to try other scenes (not \"01\", because there are not enough matches)\n",
    "scene_params, img0, img1 = load_scene_params2(scene_id=\"06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e654c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the two camera views side by side\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot first image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img0)\n",
    "plt.title(\"Camera 0\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot second image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img1)\n",
    "plt.title(\"Camera 1\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "points0, points1 = superglue_correspondences(img0, img1, kth=0.001, mth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Uncomment the following codeblock if you want to use perfect correspondences, computed from the ground truth 3D \n",
    "# mesh, instead of the correspondences found by superpoint and superglue. \n",
    "# This includes also matches which cannot be visible on both images, as they are on the back side of the object etc.\n",
    "# You can use this to test your reconstruction code.\n",
    "##############################\n",
    "\n",
    "# SAMPLES = 10000\n",
    "# p3d = np.random.randint(0, len(mesh.vertices), SAMPLES)\n",
    "# p3d = mesh.vertices[p3d]\n",
    "# p3d_hom = np.hstack([p3d, np.ones((SAMPLES, 1))])\n",
    "# P0 = scene_params[\"P0\"] # camera 0 projection matrix\n",
    "# P1 = scene_params[\"P1\"] # camera 1 projection matrix\n",
    "# p2d_hom0 = p3d_hom @ P0.T\n",
    "# p2d_hom1 = p3d_hom @ P1.T\n",
    "# points0 = p2d_hom0[:, :2] / p2d_hom0[:, 2][:, None]\n",
    "# points1 = p2d_hom1[:, :2] / p2d_hom1[:, 2][:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb471d",
   "metadata": {},
   "source": [
    "Let us visualize the 2D-2D correspondences below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_image = draw_matches(img0, img1, points0, points1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(matched_image)\n",
    "plt.title(\"Correspondences\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318fd44b",
   "metadata": {},
   "source": [
    "## Normalized 8-Point Algorithm (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a80b013",
   "metadata": {},
   "source": [
    "Next we implement the normalized 8-point algorithm to estimate the fundamental matrix from the 2D-2D correspondences.\n",
    "For this we first have to normalize the points such that their centroid (mean) is at the origin and the average distance to the origin is around $\\sqrt{2}$.\n",
    "Implement the function `normalize_points` that takes a set of 2D points and returns the normalized points and the transformation matrix $\\mathbf{T}$ that was used to normalize the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a23bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_points(points):\n",
    "    \"\"\"\n",
    "    Normalize 2D points for improved numerical stability.\n",
    "    \n",
    "    Parameters:\n",
    "        points (np.ndarray): 2D points, shape (n, 2)\n",
    "        \n",
    "    Returns:\n",
    "        normalized_points (np.ndarray): Normalized points, shape (n, 2)\n",
    "        T (np.ndarray): Transformation matrix, shape (3, 3)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71787b80",
   "metadata": {},
   "source": [
    "You can test your function by calling the function `test_normalize_points`. It will show the original 2D image points and the normalized image points next to each other. The normalized points should be centered around the origin and have an average distance of $\\sqrt{2}$, while keeping the same relative structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1142120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_points0, T0 = normalize_points(points0)\n",
    "norm_points1, T1 = normalize_points(points1)\n",
    "\n",
    "test_normalize_points(points0, norm_points0)\n",
    "test_normalize_points(points1, norm_points1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5379d856",
   "metadata": {},
   "source": [
    "Next, implement the normalized 8-point algorithm in the function `eight_point`. The function should take the original image points, and return the estimated fundamental matrix $\\mathbf{F}$. Make sure to make use of the normalization function you implemented before. Also make sure to enforce the rank-2 constraint on the fundamental matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b795cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eight_point(p0, p1):\n",
    "    \"\"\"\n",
    "    Normalized 8-point algorithm for fundamental matrix estimation.\n",
    "    \n",
    "    Parameters:\n",
    "        p0 (np.ndarray): Points in image 0, shape (n, 2) where n >= 8\n",
    "        p1 (np.ndarray): Corresponding points in image 1, shape (n, 2) where n >= 8\n",
    "        \n",
    "    Returns:\n",
    "        F (np.ndarray): Fundamental matrix of shape (3, 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement this function\n",
    "    \n",
    "    \n",
    "\n",
    "F = eight_point(points0, points1)\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd0fd0",
   "metadata": {},
   "source": [
    "Let us estimate the fundamental matrix from the correspondences, and compare the epipoles and epipolar lines with the ground-truth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d93e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_epipolar_lines(img0, points0, img1, points1, F):\n",
    "    epiline_img0 = img0.copy()\n",
    "    epiline_img1 = img1.copy()\n",
    "    U, S, Vt = np.linalg.svd(F)\n",
    "    epi0 = U[:, 2]\n",
    "    epi0 /= epi0[2]\n",
    "    epi1 = Vt[-1]\n",
    "    epi1 /= epi1[2]\n",
    "    for i in range(len(points0)):\n",
    "        l0 = F @ np.hstack([points1[i], 1])\n",
    "        l1 = F.T @ np.hstack([points0[i], 1])\n",
    "        epiline_img0 = draw_line(epiline_img0, l0, color=(255, 0, 0, 255), width=2)\n",
    "        epiline_img1 = draw_line(epiline_img1, l1, color=(255, 0, 0, 255), width=2)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(epiline_img0)\n",
    "    plt.title(\"Epipolar line in image 0\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(epiline_img1)\n",
    "    plt.title(\"Epipolar line in image 1\")\n",
    "    plt.show()\n",
    "\n",
    "draw_epipolar_lines(img0, points0[:10], img1, points1[:10], F)\n",
    "\n",
    "# Ground truth comparison\n",
    "_, F_gt = compute_E_F(scene_params)\n",
    "draw_epipolar_lines(img0, points0[:10], img1, points1[:10], F_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790bf948",
   "metadata": {},
   "source": [
    "What are the estimated epipoles? What are the real epipoles?\n",
    "- Estimated: \n",
    "- Real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute epipoles here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54873c62",
   "metadata": {},
   "source": [
    "Finally, compute the estimate for the essential matrix $\\mathbf{E}$ from the fundamental matrix $\\mathbf{F}$ and the camera intrinsics $\\mathbf{K}_1$ and $\\mathbf{K}_2$. The $\\mathbf{K}_i$ are part of the scene parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute essential matrix\n",
    "E = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f272a",
   "metadata": {},
   "source": [
    "## Recovering Relative Pose (5 Points)\n",
    "From the lecture you know that from the essential matrix $E$ we can recover the relative pose of the cameras. There are four possible solutions for the relative pose, but in only one of them, the scene is in front of both cameras. Implement the function `recover_relative_pose` that takes the essential matrix and returns the four possible relative poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527407a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_candidates(E):\n",
    "    \"\"\"\n",
    "    Compute the four possible camera poses from the essential matrix.\n",
    "    \n",
    "    Parameters:\n",
    "        E (np.ndarray): Essential matrix of shape (3, 3)\n",
    "        \n",
    "    Returns:\n",
    "        Candidates (list): List of 4 possible camera poses, each represented as a list [R, t]\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement this function\n",
    "    \n",
    "\n",
    "candidates = pose_candidates(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91639c",
   "metadata": {},
   "source": [
    "We can visualize the relative poses by drawing the camera frustums using `draw_candidates`. Note that each frustum has a little triangle on top pointing upwards in image frame! One of the candidates should be oriented corretly relatively to the camera 0.\n",
    "Hint for the viewer: You can use Ctrl + Left Click to rotate the viewer's camera around it's viewing axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc097b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_candidates(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eebe95",
   "metadata": {},
   "source": [
    "We know that that the correct relative pose is among these four candidates. To find the best one, we can triangulate the 3D points from the 2D-2D correspondences, and check which configuration gives us the most points in front of both cameras. Implement the function `find_best_candidate` that takes the candidates, 2D-2D correspondences and the scene params as input and returns the best candidate, as well as the triangulated 3D points. \n",
    "You can use `cv2.triangulatePoints` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_candidate(candidates, points0, points1, scene_params):\n",
    "    \"\"\"\n",
    "    Find the best candidate pose by triangulating points and counting the number of points in front of both cameras.\n",
    "    Parameters:\n",
    "        candidates (list): List of candidate poses\n",
    "        points0 (np.ndarray): Points in image 0, shape (n, 2)\n",
    "        points1 (np.ndarray): Points in image 1, shape (n, 2)\n",
    "        scene_params (dict): Scene parameters including camera intrinsics and extrinsics\n",
    "    Returns:\n",
    "        best_candidate (tuple): Best candidate pose (R, t)\n",
    "        estimated_point_cloud (np.ndarray): Estimated 3D point cloud, shape (n, 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement this function\n",
    "    \n",
    "\n",
    "best_candidate, estimated_point_cloud = find_best_candidate(candidates, points0, points1, scene_params)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86556cfa",
   "metadata": {},
   "source": [
    "Finally, let us plot the estimated relative camera poses together with the triangulated 3D points. Use the function `draw_pair_with_reconstruction` which takes as input:\n",
    "- $\\mathbf{K}_0$,\n",
    "- $\\mathbf{K}_1$,\n",
    "- the extrinsic parameters of camera 1 (recall that we assume that camera 0's frame = world frame),\n",
    "- the two images (for sizes of image planes and colors),\n",
    "- 2D points in image 0 and 1, to extract color information,\n",
    "- the triangulated 3D points.\n",
    "\n",
    "You only have to pass the correct arguments to the function. The function will plot the two images with the triangulated 3D points and the camera frustums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fecb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the extrinsic matrix RT\n",
    "RT = ...\n",
    "\n",
    "draw_pair_with_reconstruction(scene_params[\"K0\"], \n",
    "                              scene_params[\"K1\"], \n",
    "                              RT, \n",
    "                              img0, \n",
    "                              img1, \n",
    "                              points0, \n",
    "                              points1, \n",
    "                              estimated_point_cloud, \n",
    "                              size=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3DVision-ex1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
