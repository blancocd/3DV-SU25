{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 02 - Epipolar Geometry (14)\n",
        "\n",
        "- In this notebook, you'll embark on an exciting journey through the world of **epipolar geometry**. From understanding the fundamental matrix to visualizing epipolar lines and computing essential matrices, this exercise is packed with hands-on tasks that will deepen your understanding of 3D vision and camera geometry. Get ready to explore the mathematical backbone of stereo vision and its practical applications!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 - Epipoles, Epipolar Lines and essential matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "from utils import ReferenceFrame, Polygon, PrincipalAxis, Image, ImagePlane, \\\n",
        "    get_rotation_matrix, get_projection_matrix\n",
        "%matplotlib ipympl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Exercise 1: Understanding Epipolar Geometry and Essential Matrix (2 Points)\n",
        "\n",
        "In this exercise, you will implement several key functions related to epipolar geometry and use them to visualize the relationship between two cameras and a 3D point. Specifically, you will:\n",
        "\n",
        "1. **Implement Core Functions**:\n",
        "    - `skew(x)`: Compute the skew-symmetric matrix of a vector.\n",
        "    - `normalize_point(p)`: Dehomogenize a point (convert from homogeneous to Cartesian coordinates).\n",
        "    - `essential_matrix(P1, P2)`: Compute the essential matrix given two camera projection matrices.\n",
        "    - `lookat(eye, target, up)`: Compute the view matrix for a camera given its position, target, and up vector.\n",
        "\n",
        "2. **Visualize the Scene (partially implemented)**:\n",
        "    - Plot two cameras (purple and red) and a 3D point (green) in the 3D space.\n",
        "    - Compute the essential matrix using the two camera matrices.\n",
        "    - Use the essential matrix to calculate and plot the epipolar line as seen from the red camera.\n",
        "    - Sample and plot points along the epipolar line.\n",
        "\n",
        "---\n",
        "\n",
        "### üîß Instructions\n",
        "\n",
        "1. **Implement the Functions**:\n",
        "    - Write the `skew(x)` function to compute the skew-symmetric matrix of a 3D vector `x`.\n",
        "    - Write the `normalize_point(p)` function to convert a homogeneous point `p` into Cartesian coordinates.\n",
        "    - Write the `essential_matrix(P1, P2)` function to compute the essential matrix using the relative rotation and translation between two cameras.\n",
        "    - Write the `lookat(eye, target, up)` function to compute the view matrix for a camera.\n",
        "\n",
        "2. **Set Up the Scene**:\n",
        "    - Define two camera positions and orientations using the `lookat` function.\n",
        "    - Define a 3D point in space.\n",
        "\n",
        "3. **Compute the Essential Matrix**:\n",
        "    - Use the `essential_matrix()` function to compute the essential matrix from the two camera matrices.\n",
        "\n",
        "4. **Plot the Epipolar Line**:\n",
        "    - Use the essential matrix to compute the epipolar line in the red camera's image plane.\n",
        "    - Sample points along the epipolar line and plot them.\n",
        "\n",
        "5. **Visualize the Results**:\n",
        "    - Plot the 3D scene with the two cameras and the 3D point.\n",
        "    - Plot the epipolar line and the sampled points in the red camera's image plane.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Key Concepts\n",
        "\n",
        "- **Skew-Symmetric Matrix**: Used to represent cross products in matrix form.\n",
        "- **Essential Matrix**: Encodes the relative pose (rotation and translation) between two cameras.\n",
        "- **Epipolar Line**: The projection of a 3D point in one camera's image plane onto the other camera's image plane.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def skew(x):\n",
        "    \"\"\"Return the skew-symmetric matrix of a vector.\"\"\"\n",
        "    return # Write your code here\n",
        "\n",
        "def normalize_point(p):\n",
        "    return # Write your code here\n",
        "\n",
        "def camera_center(P):\n",
        "    \"\"\"Compute camera center from projection matrix P (null space).\"\"\"\n",
        "    # Write your code here\n",
        "    \n",
        "    return\n",
        "\n",
        "def essential_matrix(P1, P2):\n",
        "    \"\"\"\n",
        "    Compute the essential matrix from two projection matrices P1 and P2.\n",
        "\n",
        "    Use rotation and translation matrices of each camera to \n",
        "    compute an essential matrix E = [t]_x * R, where [t]_x is the skew-symmetric\n",
        "    matrix of the translation vector t, and R is the rotation matrix.\n",
        "\n",
        "    Hint: R is the rotation matrix of the second camera with respect to the first camera, \n",
        "    and t is the translation vector of the second camera with respect to the first camera.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    return\n",
        "\n",
        "def lookat(eye, target, up):\n",
        "    \"\"\"Compute the view matrix for a camera.\"\"\"\n",
        "    # Write your code here\n",
        "    return \n",
        "\n",
        "# Set up the scene parameters\n",
        "dx, dy, dz = np.eye(3) # World axes\n",
        "X = np.array([0, 2, 0, 1])\n",
        "IMAGE_HEIGTH = 1\n",
        "IMAGE_WIDTH = 1\n",
        "PX, PY = 0.5, 0.5\n",
        "MX, MY = 1.0, 1.0\n",
        "t1 = np.array([-1, 0, 0])  # Camera 1 translation\n",
        "t2 = np.array([1, 2.2, 0.97])   # Camera 2 translation\n",
        "FOCAL_LENGTH = 0.5\n",
        "\n",
        "\n",
        "P1 = lookat(t1, normalize_point(X), up=np.array([0, 0, 1]))\n",
        "P2 = lookat(t2, normalize_point(X), up=np.array([0, 0, 1]))\n",
        "\n",
        "camera_frame_1 = ReferenceFrame(origin=t1, dx=P1[0, :3] / np.linalg.norm(P1[0, :3]), dy=P1[1, :3] / np.linalg.norm(P1[1, :3]), dz=P1[2, :3] / np.linalg.norm(P1[2, :3]), name=\"Camera\")\n",
        "camera_frame_2 = ReferenceFrame(origin=t2, dx=P2[0, :3] / np.linalg.norm(P2[0, :3]), dy=P2[1, :3] / np.linalg.norm(P2[1, :3]), dz=P2[2, :3] / np.linalg.norm(P2[2, :3]), name=\"Camera\")\n",
        "\n",
        "\n",
        "Z1 = PrincipalAxis(camera_center=camera_frame_1.origin, camera_dz=camera_frame_1.dz, f=FOCAL_LENGTH)\n",
        "image_frame_1 = ReferenceFrame(origin=Z1.p - camera_frame_1.dx * PX - camera_frame_1.dy * PY,\n",
        "    dx=P1[0, :3], dy=P1[1, :3], dz=P1[2, :3], name=\"Image\")\n",
        "image_plane_1 = ImagePlane(origin=image_frame_1.origin, dx=image_frame_1.dx, dy=image_frame_1.dy,\n",
        "    heigth=IMAGE_HEIGTH, width=IMAGE_WIDTH, mx=MX, my=MY)\n",
        "\n",
        "\n",
        "Z_2 = PrincipalAxis(camera_center=camera_frame_2.origin, camera_dz=camera_frame_2.dz, f=FOCAL_LENGTH)\n",
        "image_frame_2 = ReferenceFrame(origin=Z_2.p - camera_frame_2.dx * PX - camera_frame_2.dy * PY,\n",
        "    dx=P2[0, :3], dy=P2[1, :3], dz=P2[2, :3], name=\"Image\")\n",
        "image_plane_2 = ImagePlane(origin=image_frame_2.origin, dx=image_frame_2.dx, dy=image_frame_2.dy,\n",
        "    heigth=IMAGE_HEIGTH, width=IMAGE_WIDTH, mx=MX, my=MY)\n",
        "\n",
        "# Compute projections of the 3D point onto the image planes (note, we don't use camera intrinsics for simplicity)\n",
        "x1_h =  P1 @ X\n",
        "x2_h =  P2 @ X\n",
        "x1 = normalize_point(x1_h)\n",
        "x2 = normalize_point(x2_h)\n",
        "\n",
        "# Compute epipoles by projecting the camera centers onto the other image plane (note, we don't use camera intrinsics for simplicity))\n",
        "C1 = camera_center(P1)\n",
        "C2 = camera_center(P2)\n",
        "e1 = normalize_point(P1 @ C2)\n",
        "e2 = normalize_point(P2 @ C1)\n",
        "\n",
        "\n",
        "# Compute fundamental matrix\n",
        "F = essential_matrix(P1, P2)\n",
        "\n",
        "# Compute epipolar line in image 2\n",
        "l2 = F @ np.append(x1, 1)\n",
        "\n",
        "# Sample 10 points along epipolar line in image 2\n",
        "a, b, c = l2\n",
        "dist = np.linalg.norm(x2 - e2) * np.sign(x1 - e1)\n",
        "x_vals = np.linspace(0, dist, 10)\n",
        "y_vals = -(a * x_vals + c) / b\n",
        "# Generate points for the line defined by a, b, c\n",
        "line_x_vals = np.asarray([-100, 100])\n",
        "line_y_vals = -(a * line_x_vals + c) / b\n",
        "\n",
        "# Plotting\n",
        "fig = plt.figure(figsize=(18, 6))\n",
        "ax = fig.add_subplot(131, projection='3d')\n",
        "\n",
        "camera_frame_1.draw3d(head_length=0.05)\n",
        "image_plane_1.draw3d()\n",
        "camera_frame_2.draw3d(head_length=0.05)\n",
        "image_plane_2.draw3d()\n",
        "\n",
        "# Draw camera centers\n",
        "ax.scatter(*C1[:3], color='purple', label='Camera 1')\n",
        "ax.scatter(*C2[:3], color='red', label='Camera 2')\n",
        "\n",
        "# Draw baseline\n",
        "ax.plot([C1[0], C2[0]], [C1[1], C2[1]], [C1[2], C2[2]], 'k--', label='Baseline')\n",
        "\n",
        "# Draw 3D point\n",
        "ax.scatter(*X[:3], color='green', label='3D Point')\n",
        "\n",
        "# Draw ray from camera 1 to 3D point\n",
        "ax.plot([C1[0], X[0]], [C1[1], X[1]], [C1[2], X[2]], 'g-', label='Ray to Point')\n",
        "\n",
        "ax.set_title(\"3D Scene\")\n",
        "ax.set_xlabel(\"X\")\n",
        "ax.set_ylabel(\"Y\")\n",
        "ax.set_zlabel(\"Z\")\n",
        "ax.set_xlim(-2, 2)\n",
        "ax.set_ylim(-2, 2)\n",
        "ax.set_zlim(-2, 2)\n",
        "ax.legend()\n",
        "\n",
        "# Image 2 epipolar line\n",
        "ax2 = fig.add_subplot(132)\n",
        "ax2.set_title(\"Image 1 Plane\")\n",
        "ax2.scatter(*x1, color='green', label='Projection of 3D point')\n",
        "ax2.scatter(*e1, color='red', label='Epipole in image 2')\n",
        "ax2.set_xlim(-IMAGE_WIDTH, +IMAGE_WIDTH)\n",
        "ax2.set_ylim(-IMAGE_HEIGTH, +IMAGE_HEIGTH)\n",
        "ax2.set_xlabel(\"x\")\n",
        "ax2.set_ylabel(\"y\")\n",
        "ax2.legend()\n",
        "\n",
        "\n",
        "# Image 2 epipolar line\n",
        "ax3 = fig.add_subplot(133)\n",
        "ax3.set_title(\"Image 2 Plane\")\n",
        "ax3.scatter(*x2, color='green', label='Projection of 3D point')\n",
        "ax3.scatter(*e2, color='purple', label='Epipole in image 2')\n",
        "ax3.plot(line_x_vals, line_y_vals, 'b-', label='Epipolar Line')\n",
        "ax3.scatter(x_vals, y_vals, color='blue', s=10, label='Samples on epipolar line')\n",
        "ax3.set_xlim(-IMAGE_WIDTH, +IMAGE_WIDTH)\n",
        "ax3.set_ylim(-IMAGE_HEIGTH, +IMAGE_HEIGTH)\n",
        "ax3.set_xlabel(\"x\")\n",
        "ax3.set_ylabel(\"y\")\n",
        "ax3.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Expected Result**:<br>\n",
        "![Correspondences](../data/epipolar_line_plot.png)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üëÅÔ∏è‚Äçüó®Ô∏è 2 - View Morphing\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this exercise, you'll implement the core steps of **view morphing** ‚Äî a technique that generates **intermediate views** between two images taken from different camera positions. The goal is to simulate a **virtual camera** that moves smoothly between two real ones, creating the illusion of motion and depth from static images.\n",
        "\n",
        "You'll be working with two images of the **same face captured from different viewpoints**, and progressively transforming them into a smooth morph.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß What You'll Do ‚Äì Step-by-Step\n",
        "\n",
        "1. **Feature Detection**  \n",
        "   Extract **facial keypoints** from the two input images using a face landmark detector.\n",
        "\n",
        "2. **Epipolar Geometry & Rectification**  \n",
        "   Compute the **fundamental matrix** to understand how the two views are related. Then, use it to **rectify** the images ‚Äî aligning them so that corresponding points lie on the same scanlines.\n",
        "\n",
        "3. **Morphing**  \n",
        "   Interpolate between the two **rectified images** to synthesize an intermediate view.\n",
        "\n",
        "4. **Postwarping**  \n",
        "   Apply a final **postwarp** to bring the morphed image back to a natural-looking perspective.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è Tools You Can Use\n",
        "\n",
        "You‚Äôre encouraged to use `OpenCV` to simplify the implementation. At each step, we‚Äôll suggest helpful functions ‚Äî such as:\n",
        "\n",
        "- `cv2.findFundamentalMat` for estimating geometry  \n",
        "- `cv2.stereoRectifyUncalibrated` and `cv2.warpPerspective` for rectification and warping  \n",
        "\n",
        "However, the real objective is not just to run the pipeline, but to **understand the geometry** powering each step.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Goals\n",
        "\n",
        "By the end of the exercise, you'll:\n",
        "\n",
        "- Understand **epipolar geometry** and **image rectification**\n",
        "- See how **keypoint correspondences** drive morphing\n",
        "- Get hands-on practice with **view synthesis** using real images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "#### Helper Functions and Image loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and provided functions\n",
        "\n",
        "import dlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"../data/shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "def get_face_keypoints(image):\n",
        "    \"\"\"Detect face keypoints using dlib.\"\"\"\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    faces = detector(gray)\n",
        "    if len(faces) == 0:\n",
        "        return None\n",
        "\n",
        "    shape = predictor(gray, faces[0])\n",
        "    points = np.array([[p.x, p.y] for p in shape.parts()])\n",
        "    return points\n",
        "\n",
        "\n",
        "# 1. Load images\n",
        "# img1 = cv2.imread('../data/einstein1.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "# img2 = cv2.imread('../data/einstein3.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "img1 = cv2.imread('../data/face_1.png')\n",
        "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "img2 = cv2.imread('../data/face_2.png')\n",
        "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "\n",
        "if img1.ndim == 2:\n",
        "    img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
        "if img2.ndim == 2:\n",
        "    img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Step 1: Detect Face Keypoints and Find Correspondences (2 Points)\n",
        "\n",
        "The first step of view morphing is to understand how the two input images relate to each other. To do this, we need to **identify matching points** ‚Äî that is, locations in both images that correspond to the same physical point on the face.\n",
        "\n",
        "In this case, we'll use a **facial keypoint detector** to extract corresponding landmarks from both images (such as the eyes, nose tip, mouth corners, etc.). These landmarks will serve as reliable and semantically meaningful matches.\n",
        "\n",
        "Why is this important?  \n",
        "üëâ Because **epipolar geometry** ‚Äî the relationship between the two views ‚Äî is entirely based on how 3D points are projected into both images. Without good correspondences, we can't compute the fundamental matrix, nor can we align or morph the views.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Instructions\n",
        "\n",
        "We provide you with a helper function `get_face_keypoints()` that returns a list of 2D facial landmark coordinates from a given image.\n",
        "\n",
        "Complete the code to extract them and visualize the inlier correspondences. You may also need `cv2.KeyPoint` and `cv2.drawMatches()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Get the face keypoints\n",
        "pts1 = # Write your code here\n",
        "pts2 = # Write your code here\n",
        "\n",
        "\n",
        "# Make sure both detected\n",
        "if pts1 is None or pts2 is None:\n",
        "    raise ValueError(\"Face not detected in one of the images\")\n",
        "\n",
        "# Convert points to cv2.KeyPoint objects\n",
        "keypoints1 = # Write your code here\n",
        "keypoints2 = # Write your code here\n",
        "\n",
        "# Create DMatch objects for the inliers\n",
        "matches = [cv2.DMatch(i, i, 0) for i in range(len(pts1))]\n",
        "\n",
        "# Draw the matches using cv2.drawMatches\n",
        "img_matches = # Write your code here\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(img_matches)\n",
        "plt.title(\"Inlier Matches\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Result** (It may  a little bit from run to run):<br>\n",
        "![Correspondences](../data/corresp.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üîß Step 2: Estimate the Geometry and Rectify the Views (4 Points)\n",
        "\n",
        "In this step, you‚Äôll bring in epipolar geometry to align the two views, preparing them for interpolation.\n",
        "\n",
        "You will:\n",
        "\n",
        "1. **Estimate the Fundamental Matrix** using the matched keypoints.  \n",
        "   Use `cv2.findFundamentalMat()` with the `RANSAC` method to ensure robustness to mismatches and noise.\n",
        "\n",
        "2. **Filter Inlier Matches**  \n",
        "   Keep only the correspondences that are consistent with the estimated epipolar geometry (i.e., the inliers).  \n",
        "   This is essential to avoid incorrect geometry in the next steps.\n",
        "   To do so, use the mask returned by cv2.fundFundamentalMat() to filter valid points.\n",
        "\n",
        "3. **Compute Rectification Homographies**  \n",
        "   Use `cv2.stereoRectifyUncalibrated()` to estimate two homographies‚Äî`H1` and `H2`‚Äîthat warp the input images so that:\n",
        "   - Epipolar lines become horizontal.\n",
        "   - Corresponding points lie on the same scanlines.\n",
        "\n",
        "4. **Warp the Images**  \n",
        "   Apply the rectification homographies to the input images using `cv2.warpPerspective()`.\n",
        "\n",
        "5. **Rectify the Keypoints**  \n",
        "   Implement a small helper function `rectify_points()` to warp the keypoints with their respective homographies, so they can be used in the interpolation step.\n",
        "\n",
        "6. **Visualize the Rectified Matches**  \n",
        "   Display the warped images side by side, and draw lines connecting the rectified keypoints.  \n",
        "   If the rectification worked correctly, all lines should be approximately horizontal!\n",
        "\n",
        "---\n",
        "\n",
        "You can use OpenCV functions to help you implement most of this step, but you'll need to write the `rectify_points()` function yourself. It simply applies a homography to a list of 2D points and converts the results back to Cartesian coordinates.\n",
        "\n",
        "This step prepares the input images for interpolation by aligning their geometry‚Äîso make sure it looks right before moving on!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming pts1, pts2 are inlier correspondences used to estimate F\n",
        "h, w = img1.shape[:2]\n",
        "\n",
        "# Find fundamental matrix using cv2 and RANSAC\n",
        "F, mask = # Write your code here\n",
        "\n",
        "# Filter inliers\n",
        "pts1_inliers = pts1[mask.ravel() == 1]\n",
        "pts2_inliers = pts2[mask.ravel() == 1]\n",
        "\n",
        "# Compute rectification homographies with cv2.stereoRectifyUncalibrated\n",
        "success, H1, H2 = # Write your code here\n",
        "\n",
        "assert success, \"Stereo rectification failed\"\n",
        "\n",
        "# Warp images using cv2.warpPerspective\n",
        "img1_rect = # Write your code here\n",
        "img2_rect = # Write your code here\n",
        "\n",
        "def rectify_points(pts, H):\n",
        "    \"\"\"\n",
        "    Apply an homografy H to a list of points (Nx2). \n",
        "    \n",
        "    Check cv2.convertPointsToHomogeneous documentation.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    return \n",
        "\n",
        "pts1_rect = rectify_points(pts1, H1)\n",
        "pts2_rect = rectify_points(pts2, H2)\n",
        "\n",
        "# Convert points to cv2.KeyPoint objects\n",
        "keypoints_rect_1 = # Write your code here\n",
        "keypoints_rect_2 = # Write your code here\n",
        "\n",
        "matches = [cv2.DMatch(i, i, 0) for i in range(len(pts1_rect))]\n",
        "\n",
        "# Draw the matches\n",
        "img_matches = # Write your code here\n",
        "\n",
        "# Show result\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(img_matches, cmap='gray')\n",
        "plt.title(\"Rectified Imags\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Result** (It may  a little bit from run to run):<br>\n",
        "![Correspondences](../data/corresp_warped.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üé® Step 3: Morph the View via Sparse Point Interpolation (4 Points)\n",
        "\n",
        "Now that you have the two rectified images and a set of corresponding points across them, it's time to blend the two point sets into a new one. The goal of this step is to simulate a virtual camera placed **somewhere between** the original cameras.\n",
        "\n",
        "Since we only have **sparse correspondences** (keypoints), the best we can do at this stage is to morph only those points and their immediate surroundings. To morph the entire image, we would need **dense correspondences**, which involve finding a mapping for every pixel between the two images. This will be addressed later in the exercise when we will exploit triangulation to find correspondencies.\n",
        "\n",
        "To morph our keypoints, you have to interpolate both:\n",
        "\n",
        "- The **positions** of the keypoints (so that each point smoothly moves from the left image to the right one).\n",
        "- The **pixel values** at each keypoint (so the appearance blends as the geometry morphs).\n",
        "\n",
        "#### üëá What You‚Äôll Do\n",
        "\n",
        "You‚Äôll implement a function called `morph_view()` that performs this interpolation.\n",
        "\n",
        "Here‚Äôs how it works conceptually:\n",
        "\n",
        "1. **Interpolate the Point Locations:**  \n",
        "   For a given value of $\\alpha \\in [0, 1]$, we compute an interpolated point as:  \n",
        "   $\\text{pts\\_interp} = (1 - \\alpha) * \\text{pts1} + \\alpha * \\text{pts2}$  \n",
        "   When $\\alpha=0$, we stay at the left view; when $\\alpha=1$, we get the right view.\n",
        "\n",
        "2. **Sample Pixel Values:**  \n",
        "   The, using the same alpha, interpolate the color at that location in both input images (using bilinear sampling).\n",
        "\n",
        "3. **Paint the Result:**  \n",
        "   We then paste the interpolated color at the interpolated location in the output image.\n",
        "\n",
        "üí° Note: This is a **sparse morphing**, which only fills pixels around the interpolated keypoints. In the next steps, you could try to densify this morph using triangulation - but for now, this is a great first approximation.\n",
        "\n",
        "You should implement the `morph_view()` function yourself. The code that calls this function and displays the morphed frames will be provided.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def morph_view(img1, img2, pts1, pts2, alpha):\n",
        "    \"\"\"\n",
        "    Interpolates between two rectified images using corresponding points.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    return\n",
        "\n",
        "frames = []\n",
        "for a in np.linspace(0, 1, 10):\n",
        "    frame = morph_view(img1_rect, img2_rect, pts1_rect, pts2_rect, alpha=a)\n",
        "    frames.append(frame)\n",
        "\n",
        "# Display the morphed images\n",
        "fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.imshow(frames[i])\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"Alpha: {i/9:.2f}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Result** (It may  a little bit from run to run):<br>\n",
        "![Correspondences](../data/morphed_keypoints.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üß© Step 4: Dense Reconstruction and Face Morphing with Delaunay Triangulation\n",
        "\n",
        "In this step, we use **Delaunay triangulation** to achieve a dense reconstruction of the face and morph the entire face between the two images. This section does not contain missing code to compelte. However, you should read and understand the provided code, and then answer some questions.\n",
        "\n",
        "Delaunay triangulation divides the set of corresponding points into triangles, ensuring that no point lies inside the circumcircle of any triangle. This property makes it ideal for warping and morphing tasks.\n",
        "\n",
        "![Delaunay Triangulation 1](../data/delan_triang_1.png)\n",
        "![Delaunay Triangulation 2](../data/delan_triang_2.png)\n",
        "\n",
        "---\n",
        "\n",
        "### üìù Questions to Consider\n",
        "\n",
        "1. **Why is Delaunay triangulation used for this task?**  \n",
        "    *answer*\n",
        "\n",
        "2. **What role does the affine transformation play in morphing?**  \n",
        "    *answer*\n",
        "\n",
        "3. **How does the `alpha` parameter affect the output?**  \n",
        "    *answer*\n",
        "\n",
        "4. **What would happen if the triangulation was not consistent between the two images (like the one in the pictures above)?**  \n",
        "    *answer*\n",
        "\n",
        "---\n",
        "\n",
        "This code provides a complete pipeline for dense face morphing, but understanding the underlying concepts is key to appreciating how it works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.spatial import Delaunay\n",
        "\n",
        "def get_delaunay_triangles(points, size):\n",
        "    \"\"\"\n",
        "    Run Delaunay and return the triangles.\n",
        "    \"\"\"\n",
        "    subdiv = Delaunay(points)\n",
        "    return subdiv.simplices  # array (M, 3) of indices\n",
        "\n",
        "def affine_transform(src, dst):\n",
        "    \"\"\"\n",
        "    Runs affine transformation 2x3 from src -> dst.\n",
        "    src and dst must be 3x2 (triangle vertices).\n",
        "    \"\"\"\n",
        "    return cv2.getAffineTransform(np.float32(src), np.float32(dst))\n",
        "\n",
        "def morph_triangle(img1, img2, out_img, tri1, tri2, tri, alpha):\n",
        "    \"\"\"\n",
        "    Warp and interpolate a triangle from img1 and img2 to out_img.\n",
        "    \"\"\"\n",
        "    # Bounding box\n",
        "    r = cv2.boundingRect(np.float32([tri]))\n",
        "    x, y, w, h = r\n",
        "    if w == 0 or h == 0:\n",
        "        return\n",
        "\n",
        "    # Offset points\n",
        "    tri1_rect = [(p[0]-x, p[1]-y) for p in tri1]\n",
        "    tri2_rect = [(p[0]-x, p[1]-y) for p in tri2]\n",
        "    tri_rect = [(p[0]-x, p[1]-y) for p in tri]\n",
        "\n",
        "    # Mask\n",
        "    mask = np.zeros((h, w, 3), dtype=np.float32)\n",
        "    cv2.fillConvexPoly(mask, np.int32(tri_rect), (1.0, 1.0, 1.0), 16, 0)\n",
        "\n",
        "    # Warp triangle 1 and 2\n",
        "    img1_crop = img1[y:y+h, x:x+w]\n",
        "    img2_crop = img2[y:y+h, x:x+w]\n",
        "\n",
        "    warp1 = cv2.warpAffine(img1_crop, affine_transform(tri1_rect, tri_rect), (w, h))\n",
        "    warp2 = cv2.warpAffine(img2_crop, affine_transform(tri2_rect, tri_rect), (w, h))\n",
        "\n",
        "    img_morphed = (1 - alpha) * warp1 + alpha * warp2\n",
        "\n",
        "    # Write on out_img\n",
        "    if x + w > out_img.shape[1]:\n",
        "        w = out_img.shape[1] - x\n",
        "        mask = mask[:h, :w]\n",
        "        img_morphed = img_morphed[:h, :w]\n",
        "    if y + h > out_img.shape[0]:\n",
        "        h = out_img.shape[0] - y\n",
        "        mask = mask[:h, :w]\n",
        "        img_morphed = img_morphed[:h, :w]\n",
        "        \n",
        "    out_img[y:y+h, x:x+w] = out_img[y:y+h, x:x+w] * (1 - mask) + img_morphed * mask\n",
        "\n",
        "def morph_image(img1, img2, pts1, pts2, alpha):\n",
        "    img1 = np.float32(img1)\n",
        "    img2 = np.float32(img2)\n",
        "    h, w = img1.shape[:2]\n",
        "    out_img = np.zeros((h, w, 3), dtype=np.float32)\n",
        "\n",
        "    pts = (1 - alpha) * pts1 + alpha * pts2\n",
        "    triangles = get_delaunay_triangles(pts, (w, h))\n",
        "    \n",
        "    for tri in triangles:\n",
        "        a, b, c = tri\n",
        "        morph_triangle(\n",
        "            img1, img2, out_img,\n",
        "            pts1[[a, b, c]], pts2[[a, b, c]], pts[[a, b, c]],\n",
        "            alpha\n",
        "        )\n",
        "\n",
        "    return np.clip(out_img, 0, 255).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def transform_points(points, H_inv):\n",
        "    z = np.ones(shape=(1, len(points)))\n",
        "    points = np.concatenate((points, z.T), axis=1)\n",
        "    points_trans = np.matmul(H_inv, points.T)\n",
        "    return np.array(points_trans.T).astype(np.uint8)\n",
        "\n",
        "\n",
        "def mapDelaunay(triangles_A, points_A, points_B, points_C):\n",
        "    # Using delauny triangle map for one image, get triangles for the two other images as well\n",
        "    triangles_B = []\n",
        "    triangles_C = []\n",
        "    points_A = np.uint8(points_A)\n",
        "    triangles_A = np.uint8(triangles_A)\n",
        "    for tri in triangles_A:\n",
        "        tri_B = []\n",
        "        tri_C = []\n",
        "        for i in range(0, 6, 2):\n",
        "            index = np.where(points_A==tri[i])\n",
        "            for idx in index[0]:\n",
        "                if points_A[idx][1] == tri[1+i]:\n",
        "                    tri_B.extend(points_B[idx])\n",
        "                    tri_C.extend(points_C[idx])\n",
        "\n",
        "\n",
        "        triangles_B.append(tri_B)\n",
        "        triangles_C.append(tri_C)\n",
        "\n",
        "    return triangles_B, triangles_C\n",
        "\n",
        "\n",
        "def applyAffineTransform(im_src, t_scr, dest_scr, size):\n",
        "    \"\"\"\n",
        "    Applies affine transoformation\n",
        "    \"\"\"\n",
        "    # Given a pair of triangles, find the affine transform.\n",
        "    warpMat = cv2.getAffineTransform(np.float32(t_scr), np.float32(dest_scr))\n",
        "\n",
        "    # Apply the Affine Transform just found to the src image\n",
        "    dst = cv2.warpAffine(im_src, warpMat, (size[0], size[1]), None, flags=cv2.INTER_LINEAR,\n",
        "                         borderMode=cv2.BORDER_REFLECT_101)\n",
        "\n",
        "    return dst\n",
        "\n",
        "def remove_points(points_1, points_2, remove_idx=None):\n",
        "    # Remove some of the redundant poits\n",
        "    if remove_idx is None:\n",
        "        remove_idx = [1, 3, 5, 7, 9, 11, 13, 15, 50, 60, 61, 62, 63, 64, 66, 54, 65, 56, 33, 35]\n",
        "    points_1 = np.delete(points_1, remove_idx, 0)\n",
        "    points_2 = np.delete(points_2, remove_idx, 0)\n",
        "\n",
        "    return points_1, points_2\n",
        "\n",
        "def insert_points(subdiv, p_list):\n",
        "    \"\"\"\n",
        "    Insert a list of points in a subdiv\n",
        "    \"\"\"\n",
        "    for i in p_list:\n",
        "        subdiv.insert(tuple(i))\n",
        "        \n",
        "def delaunay_triangulation(image1, image2, points_1, points_2, morphshape, removepoints=True, alph=0.5):\n",
        "    \"\"\"\n",
        "    Performs morphing of two images with delaunay triangulation. Alph indicates how much morphed image looks like image 2\n",
        "    \"\"\"\n",
        "    # Remove some points to avoid overcrowding\n",
        "    if removepoints:\n",
        "        points_1, points_2 = remove_points(points_1, points_2)\n",
        "\n",
        "    # Get intermediate points for generated image with alpha blend\n",
        "    points_k = # Write your code here\n",
        "\n",
        "    # get the Delaunay triangulation of the points (use get_delaunay_triangles),\n",
        "    # and for each triangle, get the corresponding points (build an array of Nx6 points)\n",
        "    triangles_1 = # Write your code here\n",
        "\n",
        "    # Get Delaunay triangles from the subdiv. Get the corresponding triangles for the two other \"images\"\n",
        "    triangles_2, triangles_k = mapDelaunay(triangles_1, points_1, points_2, points_k)\n",
        "\n",
        "    # initiate morphed image placeholder\n",
        "    morph_im = np.zeros(shape=np.array([morphshape[0], morphshape[1], 3]))\n",
        "\n",
        "    for i in range(0, len(triangles_k)):\n",
        "        # Find bounding rectangle\n",
        "        t1 = triangles_1[i]\n",
        "        t2 = triangles_2[i]\n",
        "        tk = triangles_k[i]\n",
        "\n",
        "        # Bounding rectangles created\n",
        "        r1 = cv2.boundingRect(np.float32([(t1[0], t1[1]),\n",
        "                                          (t1[2], t1[3]),\n",
        "                                          (t1[4], t1[5])]))\n",
        "        r2 = cv2.boundingRect(np.float32([(t2[0], t2[1]),\n",
        "                                          (t2[2], t2[3]),\n",
        "                                          (t2[4], t2[5])]))\n",
        "        rk = cv2.boundingRect(np.float32([(tk[0], tk[1]),\n",
        "                                          (tk[2], tk[3]),\n",
        "                                          (tk[4], tk[5])]))\n",
        "\n",
        "        # Get triangle position within the rectangles\n",
        "        t1Rect = []\n",
        "        t2Rect = []\n",
        "        tkRect = []\n",
        "\n",
        "        for j in range(0, 3):\n",
        "            tkRect.append(((tk[0+2*j] - rk[0]), (tk[1+2*j] - rk[1])))\n",
        "            t1Rect.append(((t1[0+2*j] - r1[0]), (t1[1+2*j] - r1[1])))\n",
        "            t2Rect.append(((t2[0+2*j] - r2[0]), (t2[1+2*j] - r2[1])))\n",
        "\n",
        "        # Apply warp to the rectangular patches\n",
        "        img1Rect = image1[r1[1]:r1[1] + r1[3], r1[0]:r1[0] + r1[2]]\n",
        "        img2Rect = image2[r2[1]:r2[1] + r2[3], r2[0]:r2[0] + r2[2]]\n",
        "\n",
        "        size = (rk[2], rk[3])\n",
        "        warpImage1 = applyAffineTransform(img1Rect, t1Rect, tkRect, size)\n",
        "        warpImage2 = applyAffineTransform(img2Rect, t2Rect, tkRect, size)\n",
        "\n",
        "\n",
        "        # Fill in the morphed image\n",
        "        imgRect = (1.0 - alph) * np.float32(warpImage1) + alph * warpImage2\n",
        "\n",
        "\n",
        "        m = morph_im[rk[1]:rk[1] + rk[3], rk[0]:rk[0] + rk[2]]\n",
        "        m_shape = m.shape\n",
        "        rect_shape = imgRect.shape\n",
        "        if m_shape != rect_shape:\n",
        "            imgRect = cv2.resize(imgRect, (m_shape[1], m_shape[0]), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # Create mask and fill the triangle\n",
        "        mask = np.zeros(imgRect.shape, dtype=np.float32)\n",
        "\n",
        "        # Fill the triangle in the mask\n",
        "        cv2.fillConvexPoly(mask, np.int32(tkRect), (1.0, 1.0, 1.0), 16, 0)\n",
        "\n",
        "\n",
        "        # Copy triangular region of the rectangular patch to the output image\n",
        "        morph_im[rk[1]:rk[1] + rk[3], rk[0]:rk[0] + rk[2]] = morph_im[rk[1]:rk[1] + rk[3], rk[0]:rk[0] + rk[2]] * (1 - mask) \\\n",
        "                                                             + imgRect * mask\n",
        "\n",
        "    return morph_im.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "frames = []\n",
        "for alpha in np.linspace(0, 1, 10):\n",
        "    frame = delaunay_triangulation(img1_rect, img2_rect, pts1_rect, pts2_rect, morphshape=img1_rect.shape, alph=alpha)\n",
        "    frames.append(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot all frames\n",
        "cols = 2\n",
        "rows = len(frames) // cols\n",
        "fig, axes = plt.subplots(cols, rows, figsize=(20, 5))\n",
        "for i, frame in enumerate(frames):\n",
        "    axes[i//rows, i%rows].imshow(frame)\n",
        "    axes[i//rows, i%rows].set_title(f\"Frame {i}\")\n",
        "    axes[i//rows, i%rows].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Expected Result** (It may  a little bit from run to run):<br>\n",
        "![Correspondences](../data/morphing.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîÑ Step 5: Postwarp the Morphed Frames (2 Points)\n",
        "\n",
        "In this final step, you'll apply a **postwarp transformation** to the morphed frames to bring them back to their original perspective. This step is necessary because the rectification process in Step 2 distorted the images to align the epipolar geometry. The postwarp step reverses this distortion, ensuring the morphed frames look natural in the original camera's perspective.\n",
        "\n",
        "---\n",
        "\n",
        "#### üëá What You'll Do\n",
        "\n",
        "1. **Apply the Inverse Rectification Homography**  \n",
        "    Use the inverse of the rectification homography (`H1`) to warp each morphed frame back to the original perspective of the first image.\n",
        "\n",
        "2. **Visualize the Postwarped Frames**  \n",
        "    Display the postwarped frames to verify that the perspective has been restored.\n",
        "\n",
        "---\n",
        "\n",
        "üí° **Key Insight**:  \n",
        "The postwarp step ensures that the final morphed images are not only geometrically accurate but also visually consistent with the original input images. Without this step, the morphed frames would appear distorted and unnatural."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols = 2\n",
        "rows = len(frames) // cols\n",
        "fig, axes = plt.subplots(cols, rows, figsize=(20, 5))\n",
        "for i, frame in enumerate(frames):\n",
        "    img_postwarped = # Write your code here\n",
        "    axes[i//rows, i%rows].imshow(img_postwarped)\n",
        "    axes[i//rows, i%rows].set_title(f\"Frame {i}\")\n",
        "    axes[i//rows, i%rows].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üèÅ Conclusion\n",
        "\n",
        "In this notebook, we explored the fascinating concepts of epipolar geometry, image rectification, and view morphing. By leveraging tools like Delaunay triangulation and affine transformations, we synthesized intermediate views between two images, creating a smooth morphing effect. This exercise not only demonstrated the power of computer vision techniques but also highlighted the importance of understanding the underlying geometry of images. By the end, we successfully reconstructed dense morphs and restored the original perspectives, achieving visually compelling results!\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "i51GvUnwSc0U"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "3DVision-ex2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
