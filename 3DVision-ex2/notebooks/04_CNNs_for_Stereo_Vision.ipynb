{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df24f695",
   "metadata": {},
   "source": [
    "# Part 04 â€“ CNNs for Stereo Vision\n",
    "- In this notebook, you'll dive into the world of learning-based stereo matching. Instead of relying on hand-crafted cost functions, you'll design and train a convolutional neural network (CNN) to directly predict disparity maps from stereo image pairs, following the 'Fast Architecture' from the paper [Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches](https://arxiv.org/abs/1510.05970). This exercise will guide you through building a small neural network, training it on synthetic data, and evaluating its ability to estimate depth. Get ready to explore how deep learning can power 3D perception!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65adba98",
   "metadata": {},
   "source": [
    "### Imports and static variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib ipympl\n",
    "\n",
    "PATCH_SIZE = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09043ac7",
   "metadata": {},
   "source": [
    "## 1) The StereoPatchTripletDataset â€“ Your Training Data Loader (5 Points)\n",
    "\n",
    "To train a neural network for stereo matching, we need a dataset that teaches the network to recognize which patches from two stereo images correspond to the same 3D point. This is exactly what the StereoPatchTripletDataset class provides.\n",
    "\n",
    "In this part, you will implement this custom PyTorch dataset to load training data for your CNN.\n",
    "\n",
    "### ðŸ‘‡ What this Dataset Does\n",
    "\n",
    "This dataset returns ***triplets*** of image patches:\n",
    "- **Anchor** â€“ a patch from the left image (view1.png)\n",
    "- **Positive** â€“ a corresponding patch from the right image (view5.png) at the correct disparity (i.e. a matching patch)\n",
    "- **Negative** â€“ a randomly chosen non-matching patch from the right image, at an incorrect disparity\n",
    "\n",
    "These triplets are ideal for training using contrastive or triplet loss, where the goal is to teach the network to bring matching patches closer in feature space and push non-matching ones apart.\n",
    "\n",
    "### ðŸ” Instructions for the implementation\n",
    "\n",
    "Letâ€™s break down what happens in the __getitem__ method:\n",
    "1) Sample selection: A random image sample directory is selected from the dataset folder (e.g., sample_00001/) containing three files:\n",
    "    - view1.png â€“ left image\n",
    "    - view5.png â€“ right image\n",
    "    - disp1.png â€“ ground truth disparity map for the left image\n",
    "\n",
    "2) Random pixel sampling: A valid pixel (x, y) is randomly chosen in the left image, ensuring it's not too close to the image borders.\n",
    "\n",
    "3) Disparity lookup: The disparity value at (x, y) is read from disp1.png. This value tells us how far the corresponding pixel is shifted in the right image along the x-axis.\n",
    "\n",
    "4) Patch extraction:\n",
    "    - The anchor patch is extracted from view1 centered at (x, y).\n",
    "    - The positive patch is taken from view5, shifted left by the disparity amount.\n",
    "    - The negative patch is a randomly chosen patch from view5, offset by a random amount from the correct position (to simulate an incorrect match).\n",
    "\n",
    "5) Sanity checks: If the chosen disparity is invalid or if any patch would go out of image bounds, the function retries by calling itself recursively.\n",
    "\n",
    "6) Normalization and tensor conversion: Each patch is normalized to [0, 1] and converted to a PyTorch tensor of shape [C, H, W].\n",
    "\n",
    "### ðŸ§  Why This is Useful\n",
    "\n",
    "This dataset allows the network to learn from contrast. It sees:\n",
    "- What a good match looks like (anchor vs. positive),\n",
    "- What a bad match looks like (anchor vs. negative),\n",
    "\n",
    "And it adjusts its internal parameters to produce features that make this distinction easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9502a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StereoPatchTripletDataset(Dataset):\n",
    "    def __init__(self, root_dir, patch_size=64, n_triplets=1000):\n",
    "        '''\n",
    "        Dataset for stereo image patch triplet generation.\n",
    "        '''\n",
    "        self.root_dir = root_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.n_triplets = n_triplets\n",
    "        self.samples = self._collect_samples()\n",
    "        \n",
    "    def set_random_generator(self, seed=42):\n",
    "        '''\n",
    "        Set a seed for reproducibility.\n",
    "        '''\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    def _collect_samples(self):\n",
    "        '''\n",
    "        Load the samples from the root directory.\n",
    "        '''\n",
    "        sample_data = []\n",
    "        for sample_dir in os.listdir(self.root_dir):\n",
    "            sample_path = os.path.join(self.root_dir, sample_dir)\n",
    "            if os.path.isdir(sample_path):\n",
    "                files = os.listdir(sample_path)\n",
    "                # For training, we need all three images\n",
    "                if all(f in files for f in [\"view1.png\", \"view5.png\", \"disp1.png\"]):\n",
    "                    view1 = cv2.imread(os.path.join(sample_path, \"view1.png\"))\n",
    "                    view5 = cv2.imread(os.path.join(sample_path, \"view5.png\"))\n",
    "                    disp1 = cv2.imread(os.path.join(sample_path, \"disp1.png\"), cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "                    sample_data.append((view1, view5, disp1))\n",
    "        return sample_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Return the number of triplets in the dataset.\n",
    "        '''\n",
    "        return self.n_triplets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Generate a triplet of patches from the stereo images.\n",
    "        The triplet consists of an anchor, a positive, and a negative patch.\n",
    "        The anchor and positive patches are from the same view, while the negative patch is from a different view.\n",
    "        The patches are randomly selected from the stereo images.\n",
    "        The triplet is returned as a tuple of three tensors: (anchor, positive, negative).\n",
    "        '''\n",
    "        # Write your code here\n",
    "\n",
    "        # Hints:\n",
    "        #   Randomly pick one sample\n",
    "        #   Randomly pick a point\n",
    "        #   Get the corresponding patches from the left image\n",
    "        #   Get the corresponding patches from the right image (use the disparity map to find the corresponding point)\n",
    "        #   Sample a negative patch from the right image\n",
    "        #   Return the triplet as a tuple of tensors\n",
    "\n",
    "\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf0be4",
   "metadata": {},
   "source": [
    "## ðŸ§© Siamese CNN for Patch Matching\n",
    "\n",
    "To solve the stereo matching problem using deep learning, we need a model that can tell whether two image patches (one from the left image, one from the right) correspond to the same 3D point. To do this, weâ€™ll use a Siamese network: a neural network architecture designed to compare two inputs by embedding them into a common feature space.\n",
    "\n",
    "### ðŸ”§ SiameseFeatureExtractor\n",
    "\n",
    "This class defines a small convolutional neural network (CNN) that extracts features from a single input patch. Hereâ€™s what it does:\n",
    "- It takes in a patch of size [3, PATCH_SIZE, PATCH_SIZE] (a small RGB patch).\n",
    "- It passes it through four convolutional layers, each followed by a ReLU activation.\n",
    "- The output is a feature map of shape [64, PATCH_SIZE, PATCH_SIZE], which is then flattened into a vector of size 64 x PATCH_SIZE x PATCH_SIZE.\n",
    "- Finally, this feature vector is L2-normalized to ensure its magnitude is 1 (important for cosine similarity).\n",
    "\n",
    "This shared network will be applied to both the left and right patches â€” hence the term \"Siamese\".\n",
    "\n",
    "### ðŸ” StereoMatchingNetwork\n",
    "\n",
    "This class defines the full stereo patch comparison network, composed of:\n",
    "- A shared SiameseFeatureExtractor for both left and right patches.\n",
    "- A cosine similarity function to measure how similar the two feature vectors are.\n",
    "\n",
    "Hereâ€™s what happens during a forward pass:\n",
    "- The left and right patches are independently processed by the Siamese CNN.\n",
    "- The resulting feature vectors are compared using cosine similarity.\n",
    "    - A value close to 1 means the patches are likely a match.\n",
    "    - A value close to -1 means they are very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d1e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseFeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # Input: [1, PATCH_SIZE, PATCH_SIZE]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1)  # Output: [64, PATCH_SIZE, PATCH_SIZE]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # Shape: [batch_size, 64, PATCH_SIZE, PATCH_SIZE]\n",
    "        x = x.contiguous().view(x.size(0), -1)  # Flatten to [batch_size, 64 x PATCH_SIZE x PATCH_SIZE]\n",
    "        x = F.normalize(x, p=2, dim=1)  # L2 normalization\n",
    "        return x\n",
    "\n",
    "class StereoMatchingNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StereoMatchingNetwork, self).__init__()\n",
    "        self.feature_extractor = SiameseFeatureExtractor()\n",
    "\n",
    "    def forward(self, patch_left, patch_right):\n",
    "        # Extract features from both patches\n",
    "        feat_left = self.feature_extractor(patch_left)\n",
    "        feat_right = self.feature_extractor(patch_right)\n",
    "        # Compute cosine similarity\n",
    "        similarity = F.cosine_similarity(feat_left, feat_right, dim=1)\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84586192",
   "metadata": {},
   "source": [
    "## ðŸ‹ï¸â€â™‚ï¸ Training the Siamese Stereo Matching Network (5 Points)\n",
    "\n",
    "Now that we have a working Siamese network architecture for comparing stereo image patches, itâ€™s time to train it using a custom triplet-based training loop.\n",
    "\n",
    "This part consists of implementing the training loop for our network. From dataset loading to computing the loss and optimizing the network, this part will explain you each step that needs to be implemented\n",
    "\n",
    "### ðŸ§® 1. Custom Hinge Loss Function\n",
    "\n",
    "You first have to implement the hinge_loss:\n",
    "$$\\text{clamp}\\left( \\text{margin} - \\text{sim\\_pos} + \\text{sim\\_neg}, \\text{min}=0 \\right)$$\n",
    "\n",
    "We use a hinge loss to train the model using triplets of patches:\n",
    "- Anchor patch from the left image.\n",
    "- Positive patch from the right image at the correct disparity.\n",
    "- Negative patch from the right image at a wrong disparity.\n",
    "The loss encourages:\n",
    "- High similarity (sim_pos) for anchorâ€“positive pairs,\n",
    "- Low similarity (sim_neg) for anchorâ€“negative pairs.\n",
    "\n",
    "\n",
    "### ðŸ“¦ 2. Initializing the Dataset & DataLoader\n",
    "We use the custom StereoPatchTripletDataset you explored earlier to sample triplets on-the-fly. The DataLoader handles:\n",
    "- Batching,\n",
    "- Running data loading in parallel (with num_workers).\n",
    "\n",
    "### ðŸ§  3. Model and Optimizer\n",
    "Create an instance of your `StereoMatchingNetwork` and move it to the GPU (if available).\n",
    "\n",
    "Use the Adam optimizer with a small learning rate (1e-4 should work) to update the networkâ€™s weights.\n",
    "\n",
    "### ðŸ” 4. Training Loop\n",
    "We repeat the training over `num_epochs`. For each epoch:\n",
    "- ðŸ“¦ Loop Over Batches\n",
    "- ðŸ“ˆ Forward Pass & Loss\n",
    "- ðŸ§® Backpropagation & Optimization\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** Training on CPU/GPU should take only a few minutes if both the dataloader and the training loop are correctly implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom hinge loss function\n",
    "def hinge_loss(sim_pos, sim_neg, margin=0.2):\n",
    "    # Write your code here\n",
    "    return \n",
    "\n",
    "n_triplets = 10000\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "lr=1e-4\n",
    "\n",
    "train_path = '../data/dataset/train'\n",
    "\n",
    "# Initialize dataset and DataLoader\n",
    "train_dataset = # Write your code here\n",
    "train_loader = # Write your code here\n",
    "\n",
    "# Initialize model and optimizer. Remember to move the model to the device if needed.\n",
    "model = # Write your code here\n",
    "optimizer = # Write your code here\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "margin = 0.2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Write your code here\n",
    "    # Hints:\n",
    "    #   Set the model to training mode\n",
    "    #   Set a different random seed for each epoch\n",
    "    #   Don't modify the running loss and progress bar as they are already set up\n",
    "    #   Iterate over the DataLoader. For each batch:\n",
    "    #       move the data to the device\n",
    "    #       zero the gradients\n",
    "    #       compute the similarities\n",
    "    #       compute the loss\n",
    "    #       backpropagation and optimization\n",
    "\n",
    "    \n",
    "    # Write your code here\n",
    "\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    pbar.set_postfix(loss=0.0)\n",
    "\n",
    "    for batch_idx, (anchor, positive, negative) in enumerate(pbar):\n",
    "        # Write your code here\n",
    "\n",
    "        # Don't forget to add the batch loss to the running loss\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd86f380",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Saving the Trained Model\n",
    "\n",
    "After training your Siamese stereo matching network, itâ€™s important to save the learned weights so you can later reuse the model without having to retrain it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"stereo_matching_model.pth\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6b68f",
   "metadata": {},
   "source": [
    "## ðŸ” Loading a Saved Model for Inference or Evaluation\n",
    "\n",
    "Once you've trained and saved your model, youâ€™ll often want to load it back â€” either to make predictions, test performance, or continue training.\n",
    "\n",
    "Hereâ€™s what each line does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StereoMatchingNetwork()\n",
    "model.load_state_dict(torch.load(\"stereo_matching_model.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5356b7b",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Let's test your results!\n",
    "This script tests whether your network has actually learned something meaningful. It does so by:\n",
    "- Taking two stereo images (left and right).\n",
    "- Running your trained model to find, for each pixel in the left image, the most similar patch in the right image.\n",
    "- The horizontal distance (disparity) between the two matching patches tells us how far the object is â€” thatâ€™s depth perception!\n",
    "\n",
    "> **Note:**: this part may take a while to run on CPU as it has to call your network on many patches. If you want to test on a smaller set of pixels, consider modifying the size of the input images from [300:600, 600:900] to what may work for you. However, remember that we wil test on the whole [300:600, 600:900] interval.\n",
    "\n",
    "### ðŸ§  Key Concepts\n",
    "What is a disparity map?\n",
    "\n",
    "A disparity map tells you, for every pixel in the left image, how much to shift horizontally to find the corresponding point in the right image. Larger disparity = closer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "half_size = PATCH_SIZE // 2\n",
    "max_disp = 200\n",
    "\n",
    "def compute_disparity_map(left_img, right_img, patch_size, max_disp, cost_fn=None, device=None):\n",
    "\n",
    "    left_img = left_img\n",
    "    right_img = right_img\n",
    "\n",
    "    h, w, _ = left_img.shape\n",
    "    disparity_map = np.zeros((h, w), dtype=np.float32)\n",
    "\n",
    "    # Convert images to tensors\n",
    "    left_img_tensor = torch.tensor(left_img).permute(2, 0, 1).unsqueeze(0).float().to(device) / 255.0\n",
    "    right_img_tensor = torch.tensor(right_img).permute(2, 0, 1).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "    pbar = tqdm(total=(h - PATCH_SIZE) * (w - PATCH_SIZE), desc=\"Computing Disparity Map\", unit=\"pixel\")\n",
    "\n",
    "    for y in range(half_size, h - half_size):\n",
    "        for x in range(half_size, w - half_size):\n",
    "            # Extract patches\n",
    "            left_patch = left_img_tensor[0, :, max(0, y - half_size):min(h, y + half_size), max(0, x - half_size):min(w, x + half_size)]\n",
    "\n",
    "            right_inputs = []\n",
    "            for d in range(max_disp):\n",
    "                patch_x = x - d\n",
    "                if patch_x < half_size:\n",
    "                    patch_x = half_size\n",
    "\n",
    "                right_inputs.append(right_img_tensor[0, :, max(0, y - half_size):min(h, y + half_size), max(0, patch_x - half_size):min(w, patch_x + half_size)])\n",
    "            right_inputs = torch.stack(right_inputs).to(device)\n",
    "            left_patch = left_patch.unsqueeze(0).to(device).repeat(right_inputs.shape[0], 1, 1, 1)\n",
    "            # Compute costs\n",
    "            with torch.no_grad():\n",
    "                costs = cost_fn(left_patch, right_inputs).cpu().numpy()\n",
    "                \n",
    "                max_cost_idx = np.argmax(costs)\n",
    "                max_cost = costs[max_cost_idx]\n",
    "\n",
    "                disp = max_cost_idx\n",
    "\n",
    "                if max_cost < 0.9: # threshold for valid disparity\n",
    "                    disp = 0\n",
    "\n",
    "            # Update disparity map\n",
    "            disparity_map[y, x] = disp\n",
    "\n",
    "            \n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    return disparity_map\n",
    "    \n",
    "\n",
    "\n",
    "left_img = cv2.imread('../data/dataset/train/artroom2/view1.png', cv2.IMREAD_COLOR_RGB)[300:600, 600:900]\n",
    "right_img = cv2.imread('../data/dataset/train/artroom2/view5.png', cv2.IMREAD_COLOR_RGB)[300:600, 600:900]\n",
    "gt = cv2.imread('../data/dataset/train/artroom2/disp5.png', cv2.IMREAD_COLOR_RGB)[300:600, 600:900]\n",
    "\n",
    "# Compute and visualize disparity map (this may take a while)\n",
    "disparity_cnn = compute_disparity_map(left_img, right_img, patch_size=PATCH_SIZE, max_disp=max_disp, cost_fn=model, device=device)\n",
    "\n",
    "plt.figure(figsize=(22, 6))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title('Disparity Map (CNN)')\n",
    "plt.imshow(disparity_cnn, cmap='plasma')\n",
    "plt.colorbar(label='Disparity')\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title('Disparity Map (GT)')\n",
    "plt.imshow(gt[:, :, 0], cmap='plasma')\n",
    "plt.colorbar(label='Disparity')\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title('RGB (right)')\n",
    "plt.imshow(left_img)\n",
    "plt.colorbar(label='Disparity')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.title('RGB (right)')\n",
    "plt.imshow(right_img)\n",
    "plt.colorbar(label='Disparity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df7c404",
   "metadata": {},
   "source": [
    "**Expected Result** (may change bit, from train to train):\n",
    "![Resulting image](../data/disp_map_expected.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77285d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_image_with_outputs(left_img, outputs):\n",
    "    h, w, _ = left_img.shape\n",
    "    x = np.arange(w)\n",
    "    y = np.arange(h)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    z = outputs\n",
    "\n",
    "    # Normalize the image for color mapping\n",
    "    img_normalized = left_img / 255.0\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot the surface\n",
    "    ax.plot_surface(x, y, z, rstride=1, cstride=1, facecolors=img_normalized, linewidth=0, antialiased=False, shade=False)\n",
    "\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z (outputs)')\n",
    "    plt.show()\n",
    "\n",
    "plot_3d_image_with_outputs(left_img, disparity_cnn)\n",
    "plot_3d_image_with_outputs(left_img, gt[:, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd0030",
   "metadata": {},
   "source": [
    "## Bonus Points: Better Models (3 Points)\n",
    "\n",
    "As you may have noticed, the results are not 100% appealing. This is because the network did not learn enough. To get the bonus points try to improve the training.\n",
    "\n",
    "Write here what you did for the bonus points:\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3DVision-ex2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
