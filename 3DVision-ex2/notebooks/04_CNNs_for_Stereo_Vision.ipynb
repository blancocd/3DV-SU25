{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df24f695",
   "metadata": {},
   "source": [
    "# Part 04 ‚Äì CNNs for Stereo Vision\n",
    "- In this notebook, you'll dive into the world of learning-based stereo matching. Instead of relying on hand-crafted cost functions, you'll design and train a convolutional neural network (CNN) to directly predict disparity maps from stereo image pairs, following the 'Fast Architecture' from the paper [Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches](https://arxiv.org/abs/1510.05970). This exercise will guide you through building a small neural network, training it on synthetic data, and evaluating its ability to estimate depth. Get ready to explore how deep learning can power 3D perception!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65adba98",
   "metadata": {},
   "source": [
    "### Imports and static variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5c030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib ipympl\n",
    "\n",
    "PATCH_SIZE = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09043ac7",
   "metadata": {},
   "source": [
    "## 1) The StereoPatchTripletDataset ‚Äì Your Training Data Loader (5 Points)\n",
    "\n",
    "To train a neural network for stereo matching, we need a dataset that teaches the network to recognize which patches from two stereo images correspond to the same 3D point. This is exactly what the StereoPatchTripletDataset class provides.\n",
    "\n",
    "In this part, you will implement this custom PyTorch dataset to load training data for your CNN.\n",
    "\n",
    "### üëá What this Dataset Does\n",
    "\n",
    "This dataset returns ***triplets*** of image patches:\n",
    "- **Anchor** ‚Äì a patch from the left image (view1.png)\n",
    "- **Positive** ‚Äì a corresponding patch from the right image (view5.png) at the correct disparity (i.e. a matching patch)\n",
    "- **Negative** ‚Äì a randomly chosen non-matching patch from the right image, at an incorrect disparity\n",
    "\n",
    "These triplets are ideal for training using contrastive or triplet loss, where the goal is to teach the network to bring matching patches closer in feature space and push non-matching ones apart.\n",
    "\n",
    "### üîç Instructions for the implementation\n",
    "\n",
    "Let‚Äôs break down what happens in the __getitem__ method:\n",
    "1) Sample selection: A random image sample directory is selected from the dataset folder (e.g., sample_00001/) containing three files:\n",
    "    - view1.png ‚Äì left image\n",
    "    - view5.png ‚Äì right image\n",
    "    - disp1.png ‚Äì ground truth disparity map for the left image\n",
    "\n",
    "2) Random pixel sampling: A valid pixel (x, y) is randomly chosen in the left image, ensuring it's not too close to the image borders.\n",
    "\n",
    "3) Disparity lookup: The disparity value at (x, y) is read from disp1.png. This value tells us how far the corresponding pixel is shifted in the right image along the x-axis.\n",
    "\n",
    "4) Patch extraction:\n",
    "    - The anchor patch is extracted from view1 centered at (x, y).\n",
    "    - The positive patch is taken from view5, shifted left by the disparity amount.\n",
    "    - The negative patch is a randomly chosen patch from view5, offset by a random amount from the correct position (to simulate an incorrect match).\n",
    "\n",
    "5) Sanity checks: If the chosen disparity is invalid or if any patch would go out of image bounds, the function retries by calling itself recursively.\n",
    "\n",
    "6) Normalization and tensor conversion: Each patch is normalized to [0, 1] and converted to a PyTorch tensor of shape [C, H, W].\n",
    "\n",
    "### üß† Why This is Useful\n",
    "\n",
    "This dataset allows the network to learn from contrast. It sees:\n",
    "- What a good match looks like (anchor vs. positive),\n",
    "- What a bad match looks like (anchor vs. negative),\n",
    "\n",
    "And it adjusts its internal parameters to produce features that make this distinction easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9502a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StereoPatchTripletDataset(Dataset):\n",
    "    def __init__(self, root_dir, patch_size=64, n_triplets=1000):\n",
    "        '''\n",
    "        Dataset for stereo image patch triplet generation.\n",
    "        '''\n",
    "        self.root_dir = root_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.n_triplets = n_triplets\n",
    "        self.samples = self._collect_samples()\n",
    "        \n",
    "    def set_random_generator(self, seed=42):\n",
    "        '''\n",
    "        Set a seed for reproducibility.\n",
    "        '''\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    def _collect_samples(self):\n",
    "        '''\n",
    "        Load the samples from the root directory.\n",
    "        '''\n",
    "        sample_data = []\n",
    "        for sample_dir in os.listdir(self.root_dir):\n",
    "            sample_path = os.path.join(self.root_dir, sample_dir)\n",
    "            if os.path.isdir(sample_path):\n",
    "                files = os.listdir(sample_path)\n",
    "                # For training, we need all three images\n",
    "                if all(f in files for f in [\"view1.png\", \"view5.png\", \"disp1.png\"]):\n",
    "                    view1 = cv2.imread(os.path.join(sample_path, \"view1.png\"))\n",
    "                    view5 = cv2.imread(os.path.join(sample_path, \"view5.png\"))\n",
    "                    disp1 = cv2.imread(os.path.join(sample_path, \"disp1.png\"), cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "                    sample_data.append((view1, view5, disp1))\n",
    "        return sample_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Return the number of triplets in the dataset.\n",
    "        '''\n",
    "        return self.n_triplets\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        '''\n",
    "        Generate a triplet of patches from the stereo images.\n",
    "        The triplet consists of an anchor, a positive, and a negative patch.\n",
    "        The anchor and positive patches are from the same view, while the negative patch is from a different view.\n",
    "        The patches are randomly selected from the stereo images.\n",
    "        The triplet is returned as a tuple of three tensors: (anchor, positive, negative).\n",
    "        '''\n",
    "        sample_idx = random.randint(0, len(self.samples) - 1)\n",
    "        view1, view5, disp1 = self.samples[sample_idx]\n",
    "        \n",
    "        # Get image dimensions\n",
    "        h, w = view1.shape[:2]\n",
    "        half_patch = self.patch_size // 2\n",
    "        \n",
    "        # 2) Random pixel sampling: Choose a valid pixel (x, y) in the left image\n",
    "        for _ in range(100):\n",
    "            # Sample random coordinates with sufficient border padding\n",
    "            x = random.randint(half_patch, w - half_patch - 1)\n",
    "            y = random.randint(half_patch, h - half_patch - 1)\n",
    "            \n",
    "            # 3) Disparity lookup: Get disparity value at (x, y)\n",
    "            disparity = disp1[y, x]\n",
    "            \n",
    "            # Skip invalid disparities (typically 0 or very large values)\n",
    "            if disparity <= 0 or disparity > w // 4:\n",
    "                continue\n",
    "                \n",
    "            # Calculate corresponding point in right image\n",
    "            x_right = x - int(disparity)  # Disparity shifts left in right image\n",
    "            \n",
    "            # Check if positive patch would be within bounds in right image\n",
    "            if x_right < half_patch or x_right >= w - half_patch:\n",
    "                continue\n",
    "                \n",
    "            # 4) Patch extraction\n",
    "            try:\n",
    "                # Extract anchor patch from left image (view1)\n",
    "                anchor_patch = view1[y - half_patch:y + half_patch,\n",
    "                                    x - half_patch:x + half_patch]\n",
    "                \n",
    "                # Extract positive patch from right image (view5) at correct disparity\n",
    "                positive_patch = view5[y - half_patch:y + half_patch,\n",
    "                                    x_right - half_patch:x_right + half_patch]\n",
    "                \n",
    "                # Extract negative patch from right image at incorrect disparity\n",
    "                # Add random offset to create a non-matching patch\n",
    "                offset_range = max(8, int(disparity * 0.5))  # Reasonable offset based on disparity\n",
    "                offset = random.randint(-offset_range, offset_range)\n",
    "                while abs(offset) < 4:  # Ensure offset is significant enough\n",
    "                    offset = random.randint(-offset_range, offset_range)\n",
    "                    \n",
    "                x_negative = x - int(disparity) + offset\n",
    "                \n",
    "                # Ensure negative patch is within bounds\n",
    "                if x_negative < half_patch or x_negative >= w - half_patch:\n",
    "                    continue\n",
    "                    \n",
    "                negative_patch = view5[y - half_patch:y + half_patch,\n",
    "                                    x_negative - half_patch:x_negative + half_patch]\n",
    "                \n",
    "                # Verify all patches have correct dimensions\n",
    "                if (anchor_patch.shape[:2] == (self.patch_size, self.patch_size) and\n",
    "                    positive_patch.shape[:2] == (self.patch_size, self.patch_size) and\n",
    "                    negative_patch.shape[:2] == (self.patch_size, self.patch_size)):\n",
    "                    \n",
    "                    # 5) Normalization and tensor conversion\n",
    "                    # Convert BGR to RGB and normalize to [0, 1]\n",
    "                    anchor_patch = cv2.cvtColor(anchor_patch, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "                    positive_patch = cv2.cvtColor(positive_patch, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "                    negative_patch = cv2.cvtColor(negative_patch, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "                    \n",
    "                    # Convert to PyTorch tensors with shape [C, H, W]\n",
    "                    anchor_tensor = torch.from_numpy(anchor_patch.transpose(2, 0, 1))\n",
    "                    positive_tensor = torch.from_numpy(positive_patch.transpose(2, 0, 1))\n",
    "                    negative_tensor = torch.from_numpy(negative_patch.transpose(2, 0, 1))\n",
    "                    \n",
    "                    return (anchor_tensor, positive_tensor, negative_tensor)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # If patch extraction fails, continue to next attempt\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf0be4",
   "metadata": {},
   "source": [
    "## üß© Siamese CNN for Patch Matching\n",
    "\n",
    "To solve the stereo matching problem using deep learning, we need a model that can tell whether two image patches (one from the left image, one from the right) correspond to the same 3D point. To do this, we‚Äôll use a Siamese network: a neural network architecture designed to compare two inputs by embedding them into a common feature space.\n",
    "\n",
    "### üîß SiameseFeatureExtractor\n",
    "\n",
    "This class defines a small convolutional neural network (CNN) that extracts features from a single input patch. Here‚Äôs what it does:\n",
    "- It takes in a patch of size [3, PATCH_SIZE, PATCH_SIZE] (a small RGB patch).\n",
    "- It passes it through four convolutional layers, each followed by a ReLU activation.\n",
    "- The output is a feature map of shape [64, PATCH_SIZE, PATCH_SIZE], which is then flattened into a vector of size 64 x PATCH_SIZE x PATCH_SIZE.\n",
    "- Finally, this feature vector is L2-normalized to ensure its magnitude is 1 (important for cosine similarity).\n",
    "\n",
    "This shared network will be applied to both the left and right patches ‚Äî hence the term \"Siamese\".\n",
    "\n",
    "### üîÅ StereoMatchingNetwork\n",
    "\n",
    "This class defines the full stereo patch comparison network, composed of:\n",
    "- A shared SiameseFeatureExtractor for both left and right patches.\n",
    "- A cosine similarity function to measure how similar the two feature vectors are.\n",
    "\n",
    "Here‚Äôs what happens during a forward pass:\n",
    "- The left and right patches are independently processed by the Siamese CNN.\n",
    "- The resulting feature vectors are compared using cosine similarity.\n",
    "    - A value close to 1 means the patches are likely a match.\n",
    "    - A value close to -1 means they are very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "176d1e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseFeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # Input: [1, PATCH_SIZE, PATCH_SIZE]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1)  # Output: [64, PATCH_SIZE, PATCH_SIZE]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # Shape: [batch_size, 64, PATCH_SIZE, PATCH_SIZE]\n",
    "        x = x.contiguous().view(x.size(0), -1)  # Flatten to [batch_size, 64 x PATCH_SIZE x PATCH_SIZE]\n",
    "        x = F.normalize(x, p=2, dim=1)  # L2 normalization\n",
    "        return x\n",
    "\n",
    "class StereoMatchingNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StereoMatchingNetwork, self).__init__()\n",
    "        self.feature_extractor = SiameseFeatureExtractor()\n",
    "\n",
    "    def forward(self, patch_left, patch_right):\n",
    "        # Extract features from both patches\n",
    "        feat_left = self.feature_extractor(patch_left)\n",
    "        feat_right = self.feature_extractor(patch_right)\n",
    "        # Compute cosine similarity\n",
    "        similarity = F.cosine_similarity(feat_left, feat_right, dim=1)\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84586192",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÇÔ∏è Training the Siamese Stereo Matching Network (5 Points)\n",
    "\n",
    "Now that we have a working Siamese network architecture for comparing stereo image patches, it‚Äôs time to train it using a custom triplet-based training loop.\n",
    "\n",
    "This part consists of implementing the training loop for our network. From dataset loading to computing the loss and optimizing the network, this part will explain you each step that needs to be implemented\n",
    "\n",
    "### üßÆ 1. Custom Hinge Loss Function\n",
    "\n",
    "You first have to implement the hinge_loss:\n",
    "$$\\text{clamp}\\left( \\text{margin} - \\text{sim\\_pos} + \\text{sim\\_neg}, \\text{min}=0 \\right)$$\n",
    "\n",
    "We use a hinge loss to train the model using triplets of patches:\n",
    "- Anchor patch from the left image.\n",
    "- Positive patch from the right image at the correct disparity.\n",
    "- Negative patch from the right image at a wrong disparity.\n",
    "The loss encourages:\n",
    "- High similarity (sim_pos) for anchor‚Äìpositive pairs,\n",
    "- Low similarity (sim_neg) for anchor‚Äìnegative pairs.\n",
    "\n",
    "\n",
    "### üì¶ 2. Initializing the Dataset & DataLoader\n",
    "We use the custom StereoPatchTripletDataset you explored earlier to sample triplets on-the-fly. The DataLoader handles:\n",
    "- Batching,\n",
    "- Running data loading in parallel (with num_workers).\n",
    "\n",
    "### üß† 3. Model and Optimizer\n",
    "Create an instance of your `StereoMatchingNetwork` and move it to the GPU (if available).\n",
    "\n",
    "Use the Adam optimizer with a small learning rate (1e-4 should work) to update the network‚Äôs weights.\n",
    "\n",
    "### üîÅ 4. Training Loop\n",
    "We repeat the training over `num_epochs`. For each epoch:\n",
    "- üì¶ Loop Over Batches\n",
    "- üìà Forward Pass & Loss\n",
    "- üßÆ Backpropagation & Optimization\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** Training on CPU/GPU should take only a few minutes if both the dataloader and the training loop are correctly implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e1f4661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:04<00:00, 71.05batch/s, loss=0.162] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:03<00:00, 83.95batch/s, loss=0.145] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.0841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:03<00:00, 83.78batch/s, loss=0.0587]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.0787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:03<00:00, 80.10batch/s, loss=0.141] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:04<00:00, 77.84batch/s, loss=0.0691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.0725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:03<00:00, 78.75batch/s, loss=0.057] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.0744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:03<00:00, 85.99batch/s, loss=0.0786] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:03<00:00, 85.81batch/s, loss=0.0763] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.0726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:04<00:00, 77.47batch/s, loss=0.0186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.0681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:03<00:00, 80.01batch/s, loss=0.12]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.0710\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom hinge loss function\n",
    "def hinge_loss(sim_pos, sim_neg, margin=0.2):\n",
    "    \"\"\"\n",
    "    Compute hinge loss for triplet training.\n",
    "    Loss = clamp(margin - sim_pos + sim_neg, min=0)\n",
    "    \n",
    "    Args:\n",
    "        sim_pos: Similarity between anchor and positive patches\n",
    "        sim_neg: Similarity between anchor and negative patches\n",
    "        margin: Margin for the hinge loss\n",
    "    \n",
    "    Returns:\n",
    "        Hinge loss value\n",
    "    \"\"\"\n",
    "    loss = torch.clamp(margin - sim_pos + sim_neg, min=0.0)\n",
    "    return loss.mean()\n",
    "\n",
    "n_triplets = 10000\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "lr = 1e-4\n",
    "\n",
    "train_path = '../data/dataset/train'\n",
    "\n",
    "# Initialize dataset and DataLoader\n",
    "train_dataset = StereoPatchTripletDataset(\n",
    "    root_dir=train_path,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    n_triplets=n_triplets\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = StereoMatchingNetwork().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "margin = 0.2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Set a different random seed for each epoch for reproducibility\n",
    "    train_dataset.set_random_generator(seed=42 + epoch)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    pbar.set_postfix(loss=0.0)\n",
    "\n",
    "    for batch_idx, (anchor, positive, negative) in enumerate(pbar):\n",
    "        # Move data to device\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: compute similarities\n",
    "        sim_pos = model(anchor, positive)  # Similarity between anchor and positive\n",
    "        sim_neg = model(anchor, negative)  # Similarity between anchor and negative\n",
    "        \n",
    "        # Compute the hinge loss\n",
    "        loss = hinge_loss(sim_pos, sim_neg, margin=margin)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss and progress bar\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd86f380",
   "metadata": {},
   "source": [
    "## üíæ Saving the Trained Model\n",
    "\n",
    "After training your Siamese stereo matching network, it‚Äôs important to save the learned weights so you can later reuse the model without having to retrain it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "397f59c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"stereo_matching_model.pth\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6b68f",
   "metadata": {},
   "source": [
    "## üîÅ Loading a Saved Model for Inference or Evaluation\n",
    "\n",
    "Once you've trained and saved your model, you‚Äôll often want to load it back ‚Äî either to make predictions, test performance, or continue training.\n",
    "\n",
    "Here‚Äôs what each line does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e34e842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = StereoMatchingNetwork()\n",
    "model.load_state_dict(torch.load(\"stereo_matching_model.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5356b7b",
   "metadata": {},
   "source": [
    "## üéØ Let's test your results!\n",
    "This script tests whether your network has actually learned something meaningful. It does so by:\n",
    "- Taking two stereo images (left and right).\n",
    "- Running your trained model to find, for each pixel in the left image, the most similar patch in the right image.\n",
    "- The horizontal distance (disparity) between the two matching patches tells us how far the object is ‚Äî that‚Äôs depth perception!\n",
    "\n",
    "> **Note:**: this part may take a while to run on CPU as it has to call your network on many patches. If you want to test on a smaller set of pixels, consider modifying the size of the input images from [300:600, 600:900] to what may work for you. However, remember that we wil test on the whole [300:600, 600:900] interval.\n",
    "\n",
    "### üß† Key Concepts\n",
    "What is a disparity map?\n",
    "\n",
    "A disparity map tells you, for every pixel in the left image, how much to shift horizontally to find the corresponding point in the right image. Larger disparity = closer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9eb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Disparity Map:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 34431/80656 [03:20<04:25, 173.98pixel/s]"
     ]
    }
   ],
   "source": [
    "half_size = PATCH_SIZE // 2\n",
    "max_disp = 200\n",
    "\n",
    "def compute_disparity_map(left_img, right_img, patch_size, max_disp, cost_fn=None, device=None):\n",
    "\n",
    "    left_img = left_img\n",
    "    right_img = right_img\n",
    "\n",
    "    h, w, _ = left_img.shape\n",
    "    disparity_map = np.zeros((h, w), dtype=np.float32)\n",
    "\n",
    "    # Convert images to tensors\n",
    "    left_img_tensor = torch.tensor(left_img).permute(2, 0, 1).unsqueeze(0).float().to(device) / 255.0\n",
    "    right_img_tensor = torch.tensor(right_img).permute(2, 0, 1).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "    pbar = tqdm(total=(h - PATCH_SIZE) * (w - PATCH_SIZE), desc=\"Computing Disparity Map\", unit=\"pixel\")\n",
    "\n",
    "    for y in range(half_size, h - half_size):\n",
    "        for x in range(half_size, w - half_size):\n",
    "            # Extract patches\n",
    "            left_patch = left_img_tensor[0, :, max(0, y - half_size):min(h, y + half_size), max(0, x - half_size):min(w, x + half_size)]\n",
    "\n",
    "            right_inputs = []\n",
    "            for d in range(max_disp):\n",
    "                patch_x = x - d\n",
    "                if patch_x < half_size:\n",
    "                    patch_x = half_size\n",
    "\n",
    "                right_inputs.append(right_img_tensor[0, :, max(0, y - half_size):min(h, y + half_size), max(0, patch_x - half_size):min(w, patch_x + half_size)])\n",
    "            right_inputs = torch.stack(right_inputs).to(device)\n",
    "            left_patch = left_patch.unsqueeze(0).to(device).repeat(right_inputs.shape[0], 1, 1, 1)\n",
    "            # Compute costs\n",
    "            with torch.no_grad():\n",
    "                costs = cost_fn(left_patch, right_inputs).cpu().numpy()\n",
    "                \n",
    "                max_cost_idx = np.argmax(costs)\n",
    "                max_cost = costs[max_cost_idx]\n",
    "\n",
    "                disp = max_cost_idx\n",
    "\n",
    "                if max_cost < 0.9: # threshold for valid disparity\n",
    "                    disp = 0\n",
    "\n",
    "            # Update disparity map\n",
    "            disparity_map[y, x] = disp\n",
    "\n",
    "            \n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    return disparity_map\n",
    "    \n",
    "\n",
    "\n",
    "left_img = cv2.imread('../data/dataset/train/artroom2/view1.png', cv2.IMREAD_COLOR_RGB)[300:600, 600:900]\n",
    "right_img = cv2.imread('../data/dataset/train/artroom2/view5.png', cv2.IMREAD_COLOR_RGB)[300:600, 600:900]\n",
    "gt = cv2.imread('../data/dataset/train/artroom2/disp5.png', cv2.IMREAD_COLOR_RGB)[300:600, 600:900]\n",
    "\n",
    "# Compute and visualize disparity map (this may take a while)\n",
    "disparity_cnn = compute_disparity_map(left_img, right_img, patch_size=PATCH_SIZE, max_disp=max_disp, cost_fn=model, device=device)\n",
    "\n",
    "plt.figure(figsize=(22, 6))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title('Disparity Map (CNN)')\n",
    "plt.imshow(disparity_cnn, cmap='plasma')\n",
    "plt.colorbar(label='Disparity')\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title('Disparity Map (GT)')\n",
    "plt.imshow(gt[:, :, 0], cmap='plasma')\n",
    "plt.colorbar(label='Disparity')\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title('RGB (right)')\n",
    "plt.imshow(left_img)\n",
    "plt.colorbar(label='Disparity')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.title('RGB (right)')\n",
    "plt.imshow(right_img)\n",
    "plt.colorbar(label='Disparity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df7c404",
   "metadata": {},
   "source": [
    "**Expected Result** (may change bit, from train to train):\n",
    "![Resulting image](../data/disp_map_expected.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77285d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_image_with_outputs(left_img, outputs):\n",
    "    h, w, _ = left_img.shape\n",
    "    x = np.arange(w)\n",
    "    y = np.arange(h)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    z = outputs\n",
    "\n",
    "    # Normalize the image for color mapping\n",
    "    img_normalized = left_img / 255.0\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot the surface\n",
    "    ax.plot_surface(x, y, z, rstride=1, cstride=1, facecolors=img_normalized, linewidth=0, antialiased=False, shade=False)\n",
    "\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z (outputs)')\n",
    "    plt.show()\n",
    "\n",
    "plot_3d_image_with_outputs(left_img, disparity_cnn)\n",
    "plot_3d_image_with_outputs(left_img, gt[:, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd0030",
   "metadata": {},
   "source": [
    "## Bonus Points: Better Models (3 Points)\n",
    "\n",
    "As you may have noticed, the results are not 100% appealing. This is because the network did not learn enough. To get the bonus points try to improve the training.\n",
    "\n",
    "Write here what you did for the bonus points:\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "garage_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
