{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NOTE: This notebook is not mandatory to pass the exercise (8 Bonus Points)\n",
        "\n",
        "As explained in the README file, this notebook covers basic and advanced concepts of PyTorch. However, you will find some exdrcises that will grant you bonus points.\n",
        "\n",
        "It consists of two sections. The first one is on Tensor Manipulation, while the second one is on Tensor Operations and Einsum\n",
        "\n",
        "Prerequisites:\n",
        "\n",
        "- Python\n",
        "\n",
        "Authors:\n",
        "- Andrea Sanchietti\n",
        "- Niklas Berndt\n",
        "- Eyvaz Najafli\n",
        "- Based on a notebook by Prof. Emanuele Rodol√† (rodola@di.uniroma1.it) and Dr. Donato Cristomi (crisostomi@di.uniroma1.it).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhTm-wTJNJEJ"
      },
      "source": [
        "# Section 1 - Tensor Manipulation\n",
        "\n",
        "- This part covers PyTorch Tensors: creation, gpu tensors, shape manipulation, indexing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDJI_JVTPMRc"
      },
      "source": [
        "## 1.1 Introduction\n",
        "\n",
        "Many Deep Learning frameworks have emerged for python. Arguably the most notable ones in 2024 are **PyTorch**, **TensorFlow** (with keras frontend) and **Jax**.\n",
        "We will use PyTorch, which is [the leading DL framework](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/) for research and [continues to gain popularity](https://openai.com/blog/openai-pytorch/).\n",
        "\n",
        "The fundamental data structure of these frameworks is the **tensor**, which is more or less the same everywhere. _A solid understanding of how tensors work is required in deep learning_ and will definitely come in handy in other areas.\n",
        "\n",
        "This part of the exercise concernes basics of tensors and operations between tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ptdrqwQQAi0"
      },
      "source": [
        "## 1.2 Numpy\n",
        "\n",
        "The adoptive father of Python's deep learning frameworks is Numpy, the historical library which added support for large, multi-dimensional arrays and matrices to Python.\n",
        "\n",
        "As we will see, modern deep learning frameworks (and especially PyTorch) have drawn largely from Numpy's API, while at the same time overcoming its limitations such as the absence of GPU support or automatic differentiation. The student has become the master.\n",
        "\n",
        "![img](https://i.imgur.com/KaUdmee.png)\n",
        "\n",
        "We will mainly use PyTorch tensors for implementing our Deep Learning systems, but knowing how to use Numpy remains very important. Note that:\n",
        "\n",
        "- **Numpy arrays** and **PyTorch tensors** are very similar, most of the features that we will explain for PyTorch tensors apply also to Numpy arrays.\n",
        "- In real DL systems you need to constantly switch between PyTorch and Numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRzmry5qj8DD"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsfPgLP9S0gA"
      },
      "source": [
        "## 1.3 PyTorch\n",
        "\n",
        "During the exercise we'll use and learn many parts of PyTorch API.\n",
        "You should also familiarize with the [PyTorch Documentation](https://pytorch.org/docs/stable/) as it will greatly assist you.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWHsmm2OTqZK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7odUeGK8TmVh"
      },
      "source": [
        "### 1.3.1 **PyTorch Tensor**\n",
        "\n",
        "The ``Tensor`` class is very similar to numpy's ``ndarray`` and provides most of its functionality.\n",
        "\n",
        "\n",
        "However, it also has two important distinctions:\n",
        "\n",
        "- ``Tensor`` supports GPU computations.\n",
        "- ``Tensor`` may store extra information needed for back-propagation:\n",
        "  - The gradient tensor w.r.t. some variable (e.g. the loss)\n",
        "  - A node representing an operation in the computational graph that produced this tensor.\n",
        "\n",
        "\n",
        "Keep in mind:\n",
        "- Usually **tensor operations are not in-place**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkkmkYGDXFFU"
      },
      "source": [
        "#### **Tensor instantiation**\n",
        "\n",
        "A tensor represents an n-dimensional grid of values, **all of the same type**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-i2-H7QU7DH"
      },
      "outputs": [],
      "source": [
        "# Basic tensor creation from python lists\n",
        "torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LNGnqk3VbkU"
      },
      "outputs": [],
      "source": [
        "# Some other tensor construction methods\n",
        "torch.zeros((3,5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8et3bE93WLBR"
      },
      "outputs": [],
      "source": [
        "torch.ones((2,5), dtype=torch.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2Ehsm4WcML0"
      },
      "outputs": [],
      "source": [
        "torch.eye(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jokiBKanWFIE"
      },
      "outputs": [],
      "source": [
        "torch.rand((2,2))  # from which distribution are these random numbers sampled? Check the PyTorch documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLz-HYi5rd5-"
      },
      "source": [
        "**Pro tip**: Bookmark the [PyTorch docs](https://pytorch.org/docs/stable/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHV8h3LVWRaI"
      },
      "outputs": [],
      "source": [
        "torch.randint(0, 100, (3,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsf9dn4CWl2V"
      },
      "outputs": [],
      "source": [
        "t = torch.rand((3, 3))\n",
        "torch.ones_like(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiRfQEztdwqE"
      },
      "source": [
        "One can easily convert to/from Numpy tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI5VsGZJd2rV"
      },
      "outputs": [],
      "source": [
        "t = torch.rand((3, 3), dtype=torch.float32)\n",
        "t.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt4Uy9Bbd81h"
      },
      "outputs": [],
      "source": [
        "n = np.random.rand(3,3).astype(np.float16)\n",
        "torch.from_numpy(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FYkcuhs84Sp"
      },
      "source": [
        "There are many other functions available to create tensors!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPEmhFYd5TK-"
      },
      "source": [
        "> **EXERCISE**\n",
        ">\n",
        "> Create a matrix $M \\in \\mathbb{R}^{3 \\times 3}$ that is filled with 2 along the diagonal and 1 elsewhere, that is:\n",
        ">\n",
        "> $$\n",
        "m_{ij} =\n",
        "\\begin{cases}\n",
        "2 & \\text{if } i = j \\\\\n",
        "1 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAnntNcu9otu"
      },
      "outputs": [],
      "source": [
        "# üìù write your solution in this cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Output:\n",
        "\n",
        "```\n",
        "tensor([[2., 1., 1.],\n",
        "        [1., 2., 1.],\n",
        "        [1., 1., 2.]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c7X85hbWs0c"
      },
      "source": [
        "#### **Tensor properties**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XCHWcMlbhH0"
      },
      "source": [
        "The **type** of a tensor is the type of each element contained in the tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP4CyRTmXYpd"
      },
      "outputs": [],
      "source": [
        "t = torch.rand((3, 3))\n",
        "t.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBoZHIlybZUV"
      },
      "source": [
        "\n",
        "The **shape** of a tensor is a tuple of integers giving the size of the tensor along each dimension, e.g. for a matrix $M \\in \\mathbb{R}^{3 \\times 5}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjuSpqbvXbqs"
      },
      "outputs": [],
      "source": [
        "t = torch.rand((3,5))\n",
        "t.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFhAblNmbswv"
      },
      "source": [
        "The **device** of a tensor indicates the memory in which the tensor is currently stored: RAM (denoted as ``cpu``) or GPU memory (denoted as ``cuda``)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4okES1zXcy2"
      },
      "outputs": [],
      "source": [
        "t = torch.rand((3,5))\n",
        "t.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM0UCkZ49DPk"
      },
      "source": [
        "> **EXERCISE**\n",
        ">\n",
        "> Given a matrix $X \\in \\mathbb{R}^{m \\times n}$, create another matrix $Y \\in \\mathbb{R}^{m \\times 3}$ filled with ones using $X$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GJinHgLzs11"
      },
      "outputs": [],
      "source": [
        "# Exercise variables\n",
        "X = torch.rand(20,42)\n",
        "\n",
        "# Your solution:\n",
        "# Y = ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Output:\n",
        "\n",
        "```\n",
        "tensor([[1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.],\n",
        "        [1., 1., 1.]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8Wa3JFCXd7J"
      },
      "source": [
        "#### **Using the GPU**\n",
        "\n",
        "> If you don't have a gpu, the code of this section may not work\n",
        "\n",
        "Thanks to the explosion of the videogame industry in the last 50 years, the performance of the chips specialized in rendering and processing graphics --known as GPUs-- has dramatically improved.\n",
        "\n",
        "In 2007 NVidia realized the potential of parallel GPU computing outside the videogame world, and released the first version of the CUDA framework, allowing  software developers to use GPUs for general purpose processing.\n",
        "\n",
        "Graphics operations are mostly linear algebra operations, and accelerating them can turn very useful in many other fields.\n",
        "\n",
        "In 2012 Hinton et al. [demonstrated](https://en.wikipedia.org/wiki/AlexNet) the huge potential of GPUs in training deep neural networks, starting *de facto* the glorious days of deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbIogawQYBvW"
      },
      "outputs": [],
      "source": [
        "# Check if the GPU is available\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVYbgoigX_PL"
      },
      "outputs": [],
      "source": [
        "# If available use the GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhHVeolzWZQn"
      },
      "outputs": [],
      "source": [
        "t = torch.rand((3,3))\n",
        "t = t.to(device)  # Note that we are assigning back to t, otherwise t won't be updated!\n",
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Al_WAx4RWDy2"
      },
      "outputs": [],
      "source": [
        "# Construct tensors directly on the GPU memory\n",
        "t = torch.ones((5, 5), device='cuda')\n",
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuynABfrVZmD"
      },
      "outputs": [],
      "source": [
        "t = torch.rand((3,3))\n",
        "\n",
        "# Other shortcuts to transfer tensors between devices\n",
        "\n",
        "# Be careful of hardcoded cuda calls: the code will not run if a GPU is not available\n",
        "t = t.cuda()\n",
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkOX8icwVXPK"
      },
      "outputs": [],
      "source": [
        "t = t.cpu()\n",
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj0gWQ-FgsZ2"
      },
      "outputs": [],
      "source": [
        "# Utility function to print tensors nicely. We will use this all the time.\n",
        "\n",
        "from typing import Union, Sequence\n",
        "\n",
        "def print_arr(\n",
        "    *arr: Sequence[Union[torch.Tensor, np.ndarray]], prefix: str = \"\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Pretty print tensors, together with their shape and type\n",
        "\n",
        "    :param arr: one or more tensors\n",
        "    :param prefix: prefix to use when printing the tensors\n",
        "    \"\"\"\n",
        "    print(\n",
        "        \"\\n\\n\".join(\n",
        "            f\"{prefix}{str(x)} <shape: {x.shape}> <dtype: {x.dtype}>\" for x in arr\n",
        "        )\n",
        "    )\n",
        "\n",
        "t = torch.rand((3,3), dtype=torch.float32)\n",
        "print_arr(t, prefix='My tensor = ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g8l8j4deyZ2"
      },
      "source": [
        "#### **Tensor rank**\n",
        "\n",
        "In Numpy and PyTorch, the **rank of a tensor** denotes the number of dimensions. For example, any matrix is a tensor of rank 2.\n",
        "\n",
        "Don't confuse this with the rank of a matrix, which has a completely different meaning in linear algebra!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF8r6t6VTbrf"
      },
      "source": [
        "- **rank-0** tensors are just scalars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfSjfJ7bTHlZ"
      },
      "outputs": [],
      "source": [
        "t0 = torch.tensor(3, dtype=torch.double)\n",
        "\n",
        "print_arr(t0)  # notice torch.Size in the printed output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O2GLaL0SxHn"
      },
      "outputs": [],
      "source": [
        "item = t0.item()  # convert the tensor scalar to a python base type\n",
        "item, type(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPcQfgGbaEqv"
      },
      "outputs": [],
      "source": [
        "# Be careful, a non-scalar tensor cannot be converted with an .item() call\n",
        "try:\n",
        "  x = torch.ones(3).item()\n",
        "except RuntimeError as e:\n",
        "  print('Error:', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El2afq3-Sujl"
      },
      "source": [
        "- **rank-1** tensors are sequences of numbers. A sequence of length ``n`` has the shape ``(n,)``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Twmbub4VSrvt"
      },
      "outputs": [],
      "source": [
        "# A rank-1 tensor\n",
        "t1 = torch.tensor([1, 2, 3])\n",
        "\n",
        "print_arr(t1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LA-5C4kSHXv"
      },
      "outputs": [],
      "source": [
        "# A rank-1 tensor with a single scalar\n",
        "print_arr(torch.tensor([42]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEGUO2rcxtwi"
      },
      "source": [
        "PyTorch and NumPy are smart: if a tensor is not rank-0 but can be converted to a rank-0 tensor, then the .item() will work.\n",
        "\n",
        "This operation is called **broadcasting**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xuBu8bKcbgl"
      },
      "outputs": [],
      "source": [
        "# A rank-1 tensor with a single element can be converted to a rank-0 tensor\n",
        "torch.tensor([42]).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj0NVuY3Rzp0"
      },
      "source": [
        "- **rank-2** tensors have the shape ``(n, m)``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wui6OfMFPdnM"
      },
      "outputs": [],
      "source": [
        "t2 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "print_arr(t2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6dSV3eWzvm-"
      },
      "outputs": [],
      "source": [
        "# element (i,j) of a rank-2 tensor just means the j-th element of the i-th rank-1 tensor\n",
        "t2[1, 2].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hBNed-yOmmQ"
      },
      "outputs": [],
      "source": [
        "# To mimick the notion of a column vector from linear algebra, we can use a rank-2 tensor\n",
        "t_col = t1.reshape(-1, 1)\n",
        "\n",
        "print_arr(t_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qad_RigUjEtA"
      },
      "outputs": [],
      "source": [
        "# ...and similarly for row vectors\n",
        "t_row = t1.reshape(1, -1)\n",
        "\n",
        "print_arr(t_row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeGPcSYMOlv0"
      },
      "source": [
        "- **rank-k** tensors have a shape of $(n_1, \\dots, n_k)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg5bbeCIjuL_"
      },
      "outputs": [],
      "source": [
        "print_arr(torch.zeros((2, 3, 4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O68PwBGOjpe"
      },
      "outputs": [],
      "source": [
        "print_arr(torch.ones((2, 2, 2, 2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkugOzEKi3wk"
      },
      "source": [
        "> **EXERCISE**\n",
        ">\n",
        "> Build a tensor $X \\in \\mathbb{R}^{k \\times k}$ filled with zeros and the sequence $[0, ..., k-1]$ along the diagonal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_8mTH8pdzHz"
      },
      "outputs": [],
      "source": [
        "# your solution\n",
        "k = 12\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "expected output for k=12\n",
        "\n",
        "```\n",
        "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "        [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0,  5,  0,  0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0,  0,  6,  0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0,  0,  0,  7,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0,  0,  0,  0,  8,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  9,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0],\n",
        "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 11]]) <shape: torch.Size([12, 12])> <dtype: torch.int64>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dixdu5yjZ0g"
      },
      "source": [
        "> **EXERCISE**\n",
        ">\n",
        "> What is the shape of the following tensor?\n",
        ">\n",
        "> ```python\n",
        "> torch.tensor(\n",
        ">     [\n",
        ">         [[1.0, 1.0, 1.0],\n",
        ">          [1.0, 1.0, 1.0]],\n",
        ">\n",
        ">         [[1.0, 1.0, 1.0],\n",
        ">          [1.0, 1.0, 1.0]],\n",
        ">\n",
        ">         [[1.0, 1.0, 1.0],\n",
        ">          [1.0, 1.0, 1.0]],\n",
        ">\n",
        ">         [[1.0, 1.0, 1.0],\n",
        ">          [1.0, 1.0, 1.0]],\n",
        ">     ]\n",
        "> )\n",
        "> ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXIhMoIO1DJA"
      },
      "outputs": [],
      "source": [
        "# Think about it, then confirm your answer by writing code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfGGF8mmSJkq"
      },
      "source": [
        "### 1.3.2 **Changing and adding dimensions**\n",
        "\n",
        "PyTorch provides several functions to manipulate tensor shapes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i51GvUnwSc0U"
      },
      "source": [
        "#### **Transpose dimension**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1kH1K7XS6KS"
      },
      "outputs": [],
      "source": [
        "a = torch.ones((3, 5))\n",
        "a[0, -1] = 0  # index -1 denotes the last element, as in common python indexing\n",
        "print(\"a: \")\n",
        "print_arr(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUXAlWb7fJn9"
      },
      "outputs": [],
      "source": [
        "a.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqEfj7tuTAqK"
      },
      "outputs": [],
      "source": [
        "a.transpose(1, 0)  # Swap dimension 1 and 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wvUahrTS9uJ"
      },
      "outputs": [],
      "source": [
        "torch.einsum('ij -> ji', a)  # transpose using Einstein notation\n",
        "\n",
        "# We will explain the Einstein notation later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nXnyRKSfPSw"
      },
      "source": [
        "#### Transpose in k-dimensions and in Numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sww5AtujTPiJ"
      },
      "outputs": [],
      "source": [
        "a = torch.ones((2, 3, 6))\n",
        "a[1, 2, 4] = 42\n",
        "print_arr(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFezEV5uTc1W"
      },
      "outputs": [],
      "source": [
        "a.transpose(2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIYTVMyRUC1e"
      },
      "outputs": [],
      "source": [
        "torch.einsum('ijk->ikj', a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF0q66cg-M6c"
      },
      "source": [
        "Shortcuts are handy, but your code becomes less readable.\n",
        "Most of the time readability is the most important goal to aim for!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9Jshcp9Ul5O"
      },
      "source": [
        "> **NOTE**\n",
        ">\n",
        "> In Numpy the transpose function is different!\n",
        ">\n",
        "> PyTorch:\n",
        "> `torch.transpose(input, dim0, dim1) ‚Üí Tensor`\n",
        ">\n",
        "> NumPy:\n",
        "> `numpy.transpose(a, axes=None) -> numpy.ndarray`\n",
        ">\n",
        "> Compare the docs from [numpy](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html) and [pytorch](https://pytorch.org/docs/stable/generated/torch.transpose.html)\n",
        ">\n",
        "> In PyTorch the transpose swaps two dimensions. In NumPy you can specify a complete mapping to change all the dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9ktDbO5Uswj"
      },
      "outputs": [],
      "source": [
        "a = np.arange(10).reshape(2, 5)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0QGO8d_hFmQ"
      },
      "outputs": [],
      "source": [
        "a.transpose(1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LLh0Qd7U2xT"
      },
      "outputs": [],
      "source": [
        "a.transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu4YJcgAhAds"
      },
      "outputs": [],
      "source": [
        "torch.from_numpy(a).transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsU83yRwU9VW"
      },
      "outputs": [],
      "source": [
        "# The einsum is cross platform. It works with consistent semantics\n",
        "# pretty much everywhere: PyTorch, NumPy, TensorFlow, Jax, ...\n",
        "# We will see the power of einsum in the next lab\n",
        "np.einsum('ij -> ji', a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWaicg6uVIXQ"
      },
      "source": [
        "#### **Reshape**\n",
        "\n",
        "Another important feature is **reshaping** a tensor into different dimensions\n",
        "\n",
        "- We need to make sure to **preserve the same number of elements**.\n",
        "- `-1` in one of the dimensions means **\"figure it out\"**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5uTNjKI15kx"
      },
      "source": [
        "‚ùå‚ùå‚ùå Pay attention that **transposing and reshaping are two fundamentally different operations**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W01aXqIZwfCu"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(12).reshape(3,4 )\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c67xp19WwmGP"
      },
      "outputs": [],
      "source": [
        "# The classical transpose\n",
        "a.t()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EenpJs6gwoLd"
      },
      "outputs": [],
      "source": [
        "# Reshape into the transpose shape\n",
        "a.reshape(4, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoIngdEzwQNY"
      },
      "source": [
        "#### **What is `reshape` really doing?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJPn4ArY2Q6E"
      },
      "source": [
        "\n",
        "Think of the `reshape` operation as unrolling the tensor **row-wise**, to obtain a rank-1 tensor *(matlab users: matlab unrolls **column-wise**, pay attention when converting code!)*. Then it stores the values in this tensor following the specified dimensions.\n",
        "\n",
        "```python\n",
        "tensor([[ 0,  1,  2,  3],\n",
        "        [ 4,  5,  6,  7],\n",
        "        [ 8,  9, 10, 11]])\n",
        "```\n",
        "$-$ unrolling $ \\to $\n",
        "\n",
        "```python\n",
        "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
        "```\n",
        "\n",
        "Then, reading the target shape from right to left, organize the values into the dimensions:\n",
        "\n",
        "- e.g. reshape into `[4, 3]`:\n",
        "\n",
        "```python\n",
        "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
        "```\n",
        "\n",
        "$-$ organize in groups of $3$ $ \\to $\n",
        "\n",
        "```python\n",
        "tensor([[0,  1,  2],  [3,  4,  5],  [6,  7,  8],  [9, 10, 11]])\n",
        "```\n",
        "\n",
        "$-$ organize in groups of $4$ $ \\to $\n",
        "\n",
        "```python\n",
        "tensor([[ 0,  1,  2],\n",
        "        [ 3,  4,  5],\n",
        "        [ 6,  7,  8],\n",
        "        [ 9, 10, 11]])\n",
        "\n",
        "# same shape of corresponding transpose, but the values are stored differently!\n",
        "```\n",
        "\n",
        "- e.g. reshape into `[2, 2, 3]`:\n",
        "\n",
        "```python\n",
        "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
        "```\n",
        "\n",
        "$-$ organize in groups of $3$ $ \\to $\n",
        "\n",
        "```python\n",
        "tensor([[0,  1,  2],  [3,  4,  5],  [6,  7,  8],  [9, 10, 11]])\n",
        "```\n",
        "\n",
        "$-$ organize in groups of $2$ $ \\to $\n",
        "\n",
        "```python\n",
        "tensor([[[0,  1,  2],  [3,  4,  5]],  [[6,  7,  8],  [9, 10, 11]]])\n",
        "```\n",
        "\n",
        "$-$ organize in groups of $2$ $ \\to $\n",
        "\n",
        "```python\n",
        "tensor([[[ 0,  1,  2],\n",
        "         [ 3,  4,  5]],\n",
        "\n",
        "        [[ 6,  7,  8],\n",
        "         [ 9, 10, 11]]])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3U_yh07Vf7m"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(12)\n",
        "print_arr(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mqUJve9VjUQ"
      },
      "outputs": [],
      "source": [
        "a.reshape(6, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-qtMeW0VnTu"
      },
      "outputs": [],
      "source": [
        "a.reshape(2, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9miZ3XUqxvc"
      },
      "outputs": [],
      "source": [
        "a.reshape(2, 2, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rUiVJQgWibl"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  a.reshape(5, -1)\n",
        "except RuntimeError as e:\n",
        "  print('Error:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9STd1HDBWkym"
      },
      "outputs": [],
      "source": [
        "a.reshape(1, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zjpBpctWr-D"
      },
      "outputs": [],
      "source": [
        "a.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWpSkDFAWo_Q"
      },
      "outputs": [],
      "source": [
        "a.reshape(-1)  # we are flattening the rank-k tensor into a rank-1 tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS6Bg3gg-X-F"
      },
      "source": [
        "> **NOTE**\n",
        ">\n",
        "> We can add or remove dimensions of size `1` using `torch.unsqueeze` or `torch.squeeze`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuRS4c8HmUNB"
      },
      "outputs": [],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz9-eqQtAR_J"
      },
      "outputs": [],
      "source": [
        "a.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2Z05GiFmUpt"
      },
      "outputs": [],
      "source": [
        "a.unsqueeze(0).shape  # adds a new dimension at the beginning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXZUg0P_mcha"
      },
      "outputs": [],
      "source": [
        "a.unsqueeze(-1).shape  # adds a new dimension at the end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYWdXFvqrbqS"
      },
      "source": [
        "> **NOTE**\n",
        ">\n",
        "> Often the reshape does not require a physical copy of the data, but just a logical\n",
        "> reorganization.\n",
        ">\n",
        "> If you are curious about the NumPy/PyTorch tensor internals, a good starting point to learn about *strides* is this [SO answer](https://stackoverflow.com/questions/53097952/how-to-understand-numpy-strides-for-layman).\n",
        "> tldr: often you can reshape tensors by changing only its strides and shape. The strides  are the byte-separation between consecutive items for each dimension.\n",
        ">\n",
        "> To be sure to obtain a *view* of the tensor, that shares the same underlying data, you can use the `torch.view` method.\n",
        "> Its semantics is similar to `reshape`, but it works only on [`contiguous` tensors](https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2) and it guarantees that no copy will be performed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYocF4AUBJN4"
      },
      "source": [
        "> **EXERCISE**\n",
        ">\n",
        "> Given a sequence of increasing numbers from `0` to `9`, defined as:\n",
        ">\n",
        "> ```python\n",
        "> a = torch.arange(10)\n",
        "> ```\n",
        ">\n",
        "> Use only the `reshape` and `transpose` functions to obtain the following tensor from `a`:\n",
        ">\n",
        "> ```python\n",
        "> tensor([0, 2, 4, 6, 8, 1, 3, 5, 7, 9])\n",
        "> ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leLESEEVD-Eq"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(10)\n",
        "\n",
        "# Write your solution here\n",
        "\n",
        "\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjvDvo4yXAk-"
      },
      "source": [
        "#### **Concatenation**\n",
        "\n",
        "PyTorch provides many functions to manipulate tensors.\n",
        "Two of the most common functions are:\n",
        "\n",
        "- `torch.stack`: Adds a **new** dimension, and concatenates the given tensors along that dimension.\n",
        "- `torch.cat`: Concatenates the given tensors along one of the **existing** dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq1IMbFvXLPk"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(12).reshape(3, 4)\n",
        "b = torch.arange(12).reshape(3, 4) + 100\n",
        "print_arr(a, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idTJRE6oDRpi"
      },
      "outputs": [],
      "source": [
        "out = torch.stack((a, b), dim=0)\n",
        "print_arr(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsyXtEm4XRf1"
      },
      "outputs": [],
      "source": [
        "out = torch.cat((a, b), dim=0)\n",
        "print_arr(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upRrgqeZILbb"
      },
      "outputs": [],
      "source": [
        "out = torch.cat((a, b), dim=1)\n",
        "print_arr(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHQFWwzCn8k7"
      },
      "source": [
        "> **EXERCISE**\n",
        ">\n",
        "> Given a tensor $X \\in \\mathbb{R}^{3 \\times 1920 \\times 5 \\times 1080}$ reorganize it in order to obtain a tensor $Y \\in \\mathbb{R}^{5 \\times 1920 \\times 1080 \\times 3}$\n",
        ">\n",
        "> Think of $X$ as a tensor that represents $5$ RGB images of size $1080\\times 1920$. Your goal is to reorganize this tensor in a sensible (and usable) way.\n",
        ">\n",
        "> *HINT: there are different ways of solving this problem. Look at the documentation for **transpose** or **permute***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrMb_KB_ogMs"
      },
      "outputs": [],
      "source": [
        "a = torch.rand(3, 1920, 5, 1080)\n",
        "a.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2twlGrOJ0VO"
      },
      "outputs": [],
      "source": [
        "# Your solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPHnbIH_rn_C"
      },
      "source": [
        "### 1.3.3 **Tensor indexing**\n",
        "\n",
        "PyTorch offers several ways to index tensors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U68ToAl1r_dG"
      },
      "source": [
        "#### **Standard indexing**\n",
        "\n",
        "As a standard Python list, PyTorch tensors support the python indexing conventions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWzGd3vYsb8e"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(10)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTn3_9F3tAVl"
      },
      "outputs": [],
      "source": [
        "print(a[0])  # first element\n",
        "print(a[5])  # sixth element"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDGstnvrs6wf"
      },
      "outputs": [],
      "source": [
        "print(a[-1])  # last element\n",
        "print(a[-2])  # second last element"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_uRKJqor2Ty"
      },
      "source": [
        "#### **Multidimensional indexing**\n",
        "\n",
        "Since tensors may be multidimensional, you can specify **one index for each dimension**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oSg1hGQtyT8"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(10).reshape(2, 5)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfPPgXguqeT9"
      },
      "outputs": [],
      "source": [
        "a[1, 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_d7tUsVuKNM"
      },
      "outputs": [],
      "source": [
        "a[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zErRdLjud3S"
      },
      "outputs": [],
      "source": [
        "a[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j40QvNuLugWX"
      },
      "outputs": [],
      "source": [
        "a[0, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY76VPqCuhc1"
      },
      "source": [
        "#### **Slicing**\n",
        "\n",
        "Similar to Python sequences and Numpy arrays, PyTorch tensors can be easily sliced using the slice notation:\n",
        "\n",
        "```python\n",
        "a[start:stop]  # items from start to stop-1 (i.e. the last element is excluded)\n",
        "a[start:]      # items from start through the rest of the array\n",
        "a[:stop]       # items from the beginning through stop-1\n",
        "a[:]           # a shallow copy of the whole array\n",
        "```\n",
        "\n",
        "There is also an optional step value, which can be used with any of the above:\n",
        "\n",
        "```python\n",
        "a[start:stop:step] # from start to at most stop-1, by step\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku-xaZ7LvjR5"
      },
      "outputs": [],
      "source": [
        "# Sum with scalar acts element-wise\n",
        "a = torch.arange(10) + 10\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TRwjulxKo_E"
      },
      "outputs": [],
      "source": [
        "# Take the elements in positions 5..6\n",
        "a[5:7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aut0rgIEwIG_"
      },
      "outputs": [],
      "source": [
        "# Take the last 5 elements\n",
        "a[-5:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ3vLffywvQt"
      },
      "outputs": [],
      "source": [
        "# Select every element having an even index\n",
        "a[::2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMgTjK6yyRCl"
      },
      "source": [
        "With multidimensional tensors we can perform **multidimensional slicing**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJYcchmxykd6"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(10).reshape(2, 5)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkhWYbVHyq-H"
      },
      "outputs": [],
      "source": [
        "# Take the second column\n",
        "a[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PzkxXpdyMoL"
      },
      "outputs": [],
      "source": [
        "# Take the last column\n",
        "a[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q0GVL_YxVRK"
      },
      "outputs": [],
      "source": [
        "# Take a slice from the last row\n",
        "a[-1, -3:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6oeDEvEzvhQ"
      },
      "source": [
        "You can **assign** to sliced tensors, therefore *modifying the original tensor*.\n",
        "\n",
        "This means that sliced tensors are **shallow copies**: the resulting tensors **share the underlying data** with the original tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2XdMXUUzLFV"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(10).reshape(2, 5)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiM4-1Je1C3y"
      },
      "outputs": [],
      "source": [
        "b = a[0:2, 1:3]\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evbxee1A1GFo"
      },
      "outputs": [],
      "source": [
        "b[-1, :] = -999\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44A4NAYewvYd"
      },
      "outputs": [],
      "source": [
        "# The original tensor has been modified\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chqxaQU63oxh"
      },
      "outputs": [],
      "source": [
        "a[-1, -1] = -1\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_yFS2q51zdn"
      },
      "source": [
        "> **NOTE**\n",
        ">\n",
        "> Indexing with **integers yields lower rank tensors**\n",
        ">\n",
        "> Integer indexing simply means we don't use slices (:) or boolean masks for indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqLGPTtlu22k"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(12).reshape(3, 4)\n",
        "print_arr(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2vHVDozvCag"
      },
      "outputs": [],
      "source": [
        "# Rank-1 view of the second row of a\n",
        "row_r1 = a[1, :]\n",
        "print_arr(row_r1)  # notice the size of the resulting tensor, which is now lower than the original tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTsk6eG325Ib"
      },
      "outputs": [],
      "source": [
        "# Rank-2 view of the second row of a\n",
        "row_r2 = a[1:2, :]\n",
        "print_arr(row_r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUVPrn7l3A5j"
      },
      "outputs": [],
      "source": [
        "# Rank-2 view of the second row of a\n",
        "row_r3 = a[[1], :]\n",
        "print_arr(row_r3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL0qs9Gw3Mfu"
      },
      "outputs": [],
      "source": [
        "# Same with the columns\n",
        "print_arr(a[:, 1])\n",
        "print_arr(a[:, [1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUsBgzbl33Qc"
      },
      "source": [
        "#### **Integer array indexing**\n",
        "\n",
        "When we use slices (:), the resulting tensor view will always be a subarray of the original tensor.\n",
        "\n",
        "In contrast, if we index with integers only, we can construct arbitrary tensors using the data from another tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yJ7eft44Shf"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(1, 7).reshape(3, 2)\n",
        "print_arr(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDGTkvIQ4qaO"
      },
      "outputs": [],
      "source": [
        "# Example of integer array indexing\n",
        "# The returned array will have shape (3,)\n",
        "b = a[[0, 1, 2], [0, 1, 0]]\n",
        "print_arr(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8AjjBKF4nos"
      },
      "outputs": [],
      "source": [
        "# The above is equivalent to:\n",
        "v1, v2, v3 = a[0, 0], a[1, 1], a[2, 0]\n",
        "b = torch.tensor([v1, v2, v3])\n",
        "print_arr(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnVB223B5Nf9"
      },
      "outputs": [],
      "source": [
        "# You can re-use the same element of the source tensor multiple times!\n",
        "print_arr(a[[0, 0], [1, 1]])\n",
        "print_arr(torch.tensor([a[0, 1], a[0, 1]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG8PUPue5ntQ"
      },
      "outputs": [],
      "source": [
        "# You can use another tensor to perform the indexing,\n",
        "# as long as they have dtype=torch.int64 (synonym for torch.long)\n",
        "i = torch.ones(3, dtype=torch.int64)\n",
        "i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp--PCJN5uvA"
      },
      "outputs": [],
      "source": [
        "j = torch.tensor([0, 1, 0])\n",
        "j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrlnZbw25w3H"
      },
      "outputs": [],
      "source": [
        "out = a[i, j]\n",
        "\n",
        "print_arr(a, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzr5hn34sjko"
      },
      "source": [
        "> **EXERCISE**\n",
        ">\n",
        "> Using a single assignment, change the elements of a tensor $X \\in \\mathbb{R}^{4 \\times 3}$ as follows:\n",
        ">\n",
        "> `X[0,2] = -1`\n",
        ">\n",
        "> `X[1,1] = 0`\n",
        ">\n",
        "> `X[2,0] = 1`\n",
        ">\n",
        "> `X[3,1] = 2`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_3bZ5xHwmGb"
      },
      "outputs": [],
      "source": [
        "# Mutate one element from each row of a matrix\n",
        "a = torch.arange(12).reshape(4, 3)\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Output:\n",
        "\n",
        "```\n",
        "tensor([[ 0,  1, -1],\n",
        "        [ 3,  0,  5],\n",
        "        [ 1,  7,  8],\n",
        "        [ 9,  2, 11]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "544F9pAf-qRY"
      },
      "source": [
        "> ‚ùå‚ùå‚ùå **NOTE**\n",
        ">\n",
        "> **Slice indexing vs Array indexing**\n",
        ">\n",
        "> Be careful, since slice indexing and array indexing are different operations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-GYMhxA-tOY"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(16).reshape(4, 4)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Piy-JUdv-x0_"
      },
      "outputs": [],
      "source": [
        "a[0:3, 0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXJJAq9G-0rX"
      },
      "outputs": [],
      "source": [
        "a[[0, 1, 2], [0, 1, 2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKjSW5ePxzLW"
      },
      "outputs": [],
      "source": [
        "a[torch.arange(0,3), torch.arange(0,3)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkmiQgQT_k82"
      },
      "outputs": [],
      "source": [
        "a[0:5:2, 0:5:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With *slice indexing* you return a sub-tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVMo8E2W_wAw"
      },
      "source": [
        "#### **Boolean array indexing**\n",
        "\n",
        "This type of indexing is used to select the elements of a tensor that satisfy some condition (similar to MATLAB's logical indexing):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFIFupWJAnI9"
      },
      "outputs": [],
      "source": [
        "a = torch.arange(6).reshape(3, 2)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMaNbpVPAtMm"
      },
      "outputs": [],
      "source": [
        "bool_idx = (a > 2)\n",
        "bool_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbYWFo8TArod"
      },
      "outputs": [],
      "source": [
        "a[bool_idx]  # remember that NumPy and PyTorch unroll row-wise and not column-wise like Matlab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzVyXjJgBMKa"
      },
      "source": [
        "If you want to know more about indexing in PyTorch and Numpy read the [docs](https://numpy.org/doc/stable/user/basics.indexing.html#basics-indexing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSAHCIYePoB5"
      },
      "source": [
        "### 1.3.4 Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbUID2DlLuhq"
      },
      "source": [
        "> **EXERCISE**\n",
        ">\n",
        "> Build a 3D tensor in $X \\in \\mathbb{R}^{3 \\times 3 \\times 3}$ that has ones along the 3D-diagonal and zeros elsewhere, i.e. a 3D identity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trx7jDTuLe2D"
      },
      "outputs": [],
      "source": [
        "# Write here your solution\n",
        "# X = ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Output:\n",
        "\n",
        "```\n",
        "tensor([[[1., 0., 0.],\n",
        "         [0., 0., 0.],\n",
        "         [0., 0., 0.]],\n",
        "\n",
        "        [[0., 0., 0.],\n",
        "         [0., 1., 0.],\n",
        "         [0., 0., 0.]],\n",
        "\n",
        "        [[0., 0., 0.],\n",
        "         [0., 0., 0.],\n",
        "         [0., 0., 1.]]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **EXERCISE**\n",
        ">\n",
        "> Now, given a number N, build a 3D tensor in $X \\in \\mathbb{R}^{N \\times N \\times N}$ that has ones along the 3D-diagonal and zeros elsewhere, i.e. a 3D identity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write here your solution\n",
        "N = torch.randint(0, 10, (1, )).item()\n",
        "# X = ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Output for N=2:\n",
        "\n",
        "```\n",
        "tensor([[[1., 0.],\n",
        "         [0., 0.]],\n",
        "\n",
        "        [[0., 0.],\n",
        "         [0., 1.]]])\n",
        "```\n",
        "Expected Output for N=5:\n",
        "\n",
        "```\n",
        "tensor([[[1., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.]],\n",
        "\n",
        "        [[0., 0., 0., 0., 0.],\n",
        "         [0., 1., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.]],\n",
        "\n",
        "        [[0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.],\n",
        "         [0., 0., 1., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.]],\n",
        "\n",
        "        [[0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 1., 0.],\n",
        "         [0., 0., 0., 0., 0.]],\n",
        "\n",
        "        [[0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 0.],\n",
        "         [0., 0., 0., 0., 1.]]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uLHb9jWM0jM"
      },
      "source": [
        "> **EXERCISE**\n",
        ">\n",
        "> You are given a 3D tensor $X \\in \\mathbb{R}^{w \\times h \\times 3}$ representing a $w \\times h$ image with `(r, g, b)` color channels. Assume that colors take values in $[0, 1]$.\n",
        ">\n",
        "> Color the image $X$ completely by red, i.e. `(1, 0, 0)` in the `(r, g, b)` format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_HG8I5zNnpJ"
      },
      "outputs": [],
      "source": [
        "# Create and visualize a black image\n",
        "x = torch.zeros(100, 200, 3)\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "img = plt.imshow(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqqm2zZUybB7"
      },
      "outputs": [],
      "source": [
        "# Write here your solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Output:\n",
        "\n",
        "![Red Image](../data/red_image_output.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps_j8BUcQmWu"
      },
      "source": [
        "> **EXERCISE**\n",
        ">\n",
        "> You are given the GitHub logo $X \\in \\mathbb{R}^{560 \\times 560}$.  Assume the logo is in gray scale, with the color $c \\in [0, 1]$ (remember 0 $\\to$ black).\n",
        ">\n",
        "> 1. Change the black-ish color into light gray: $0.8$.\n",
        "> 2. Then draw a diagonal and anti-diagonal black line (i.e. an X) on the new image, to mark that the new logo is wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0082ZNOP-EK"
      },
      "outputs": [],
      "source": [
        "from skimage import io\n",
        "\n",
        "image = io.imread('https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png', as_gray=True)\n",
        "_ = plt.imshow(image, cmap='gray', vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thBZCe_3QyxM"
      },
      "outputs": [],
      "source": [
        "# Change the black into light-gray\n",
        "X = torch.from_numpy(image.copy())  # PyTorch CPU and Numpy share the memory!\n",
        "# Write your code here\n",
        "\n",
        "_ = plt.imshow(X, cmap='gray', vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvjjGYmkX8m7"
      },
      "outputs": [],
      "source": [
        "# Mark the new image as wrong with a big black X\n",
        "# Write your code here\n",
        "\n",
        "_ = plt.imshow(X, cmap='gray', vmin=0, vmax=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Output:\n",
        "\n",
        "![Expected Output](../data/gh_expected_result.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 2 Tensor Operations and 3D Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- This part covers Tensor operations (broadcasting, (not)-elementwise operations, tensor contraction, einsum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "In this part of the exercise we will continue to learn basic tensor usage, we will cover broadcasting, fundamental linear algebra operations, `einsum`!\n",
        "\n",
        "All these tensor operations will come in handy to build our deep neural networks.\n",
        "Yet, the high level API offered by PyTorch to perform GPU-accelerated linear algebra operations may turn useful in many other fields, from microbiology to fluid dynamics.\n",
        "\n",
        "The GPU computing paradigm offers several benefits over single-core machines or traditional supercomputers equipped with many single-core nodes.\n",
        "Deep learning frameworks such as the one we are studying are a very good compromise between simplicity and expressivenes to unleash the power of GPU-computing.\n",
        "\n",
        "To get even more control you can tackle directly the CUDA language, but we won't go there with this course!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTorch\n",
        "\n",
        "**Reminder:** Familiarize with the [PyTorch Documentation](https://pytorch.org/docs/stable/) as it will greatly assist you.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####Set torch and numpy random seeds for reproducibility\n",
        "\n",
        "As we will see, several operations in deep learning (e.g. training a network) rely on randomness in order to work effectively. This means that we will get different results each time we run a test, which can make design and debugging difficult.\n",
        "\n",
        "To this end, we usually **set a fixed seed** for the pseudo-random number generator, so that we are sure to always see the \"same randomness\" that makes our tests reproducible.\n",
        "\n",
        "> Once your model works, remember to test multiple times _without_ a fixed seed! The results you got at design time may be due to overfitting the seed (e.g. you have chosen hyperparameters that happen to work particularly well with a given seed.), or just out of luck.\n",
        "\n",
        "If you are going to use a gpu, two further options must be set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(0)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# some frameworks aid the reproducibility of your code,\n",
        "# e.g. PyTorch Lightning exposes a `seed_everything` function by default:\n",
        "# https://github.com/PyTorchLightning/pytorch-lightning/blob/e1f5eacab98670bc1de72c88657404a15aadd527/pytorch_lightning/utilities/seed.py#L29"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Tensor operations**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = torch.rand(3,3)\n",
        "t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions that operate on tensors are often accessible in different ways:\n",
        "\n",
        "- From the **`torch` module**...:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.add(t, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- ...or by tensors **methods**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t.add(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- ...or even through **overloaded** operators:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t + t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "None of the above operates in-place:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# t is unchanged\n",
        "t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These functions are all equivalent, they are *aliases* of the same method.\n",
        "Personal preference, code consistency, and readability should guide your decision of which one to use.\n",
        "\n",
        "> e.g. `torch.add(...)` may be too verbose, but in some cases it may be preferable since it makes explicit to the code-reader that you are dealing with tensors. Nevertheless, if you are using [types](https://docs.python.org/3/library/typing.html) -- and you should be using types -- it will be rarely necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Most operation in PyTorch are **not in-place**. It means that the resulting tensor is a *new* tensor, and it does not share the underlying data with other tensors. Changes to the new tensor are not reflected to other tensors.\n",
        "\n",
        "\n",
        "In-place operations are still available in PyTorch, and in some cases (e.g. when you don't need autodiff) they can be useful; they are more efficient, since they never require to perform deep copies of the data.\n",
        "They are normally recognized by a trailing `_`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t.add_(t)  # notice the trailing _"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t  # t itself changed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another common in-place operation is the assignment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t[0] = 42\n",
        "t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Basic operations and broadcasting**\n",
        "\n",
        "Basic mathematical operations $(+, -, *, /, **)$ are applied **element-wise**: for example, if `x` and `y` are two tensors, the product `x*y` is a tensor with the same size, and its values are the element-wise products of the two tensors. In mathematics, this is also called a Hadamard product.\n",
        "\n",
        "**Broadcasting** is another powerful mechanism that allows PyTorch to perform operations on tensors of different shapes. The most basic example is summing a scalar (a rank-0 tensor) to a matrix (a rank-2 tensor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float64)\n",
        "y = torch.tensor([[5, 6], [7, 8]], dtype=torch.float64)\n",
        "\n",
        "print(x + y)  # element-wise sum\n",
        "print(x + 4.2)  # broadcasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# other examples\n",
        "print(x * y - 5)\n",
        "print((x - y) / y)  # element-wise division!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Broadcasting is quite powerful! When you perform an operation between two tensors with different shape, PyTorch automatically \"broadcasts\" the smaller tensor across the larger tensor so that they have compatible shapes.\n",
        "\n",
        "In the example below, the sequence `v` is replicated (_without actually copying data!_) along the missing dimension so that it fits the shape of matrix `m`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = torch.arange(12).reshape(4, 3)\n",
        "v = torch.tensor([100, 0, 200])\n",
        "n = m + v\n",
        "print_arr(m, v, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this other example `m` and `u` are both rank-2, but the smaller one (`u`) is expanded along the dimension where it has size 1 to fit `m`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "m = torch.arange(12).reshape(4, 3)\n",
        "u = torch.tensor([0, 10, 0, 20]).reshape(4,1)\n",
        "n = m + u\n",
        "print_arr(m, u, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following example, both tensors are expanded along their size-1 dimensions, so that the sum makes sense:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w = u + v\n",
        "print_arr(u, v, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mastering broadcasting is hard!\n",
        "\n",
        "However, it is very convenient as it allows writing **vectorized** code, i.e., code that avoids explicit python loops which can not be efficiently parallelized.\n",
        "\n",
        "Technically, broadcasting takes advantage of the underlying C implementation of PyTorch and Numpy (on CPU) or CUDA implementation of Pytorch (on GPU). Here's a take-home illustration for your convenience:\n",
        "\n",
        "![broadcasting](https://jakevdp.github.io/PythonDataScienceHandbook/figures/02.05-broadcasting.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **EXERCISE**\n",
        ">\n",
        "> Given two vectors $x \\in \\mathbb{R}^n$ and $y \\in \\mathbb{R}^m$, compute the differences between all possible pairs of their elements, and organize these differences in a matrix $Z \\in \\mathbb{R}^{n \\times m}$:\n",
        "> $$ z_{ij} = x_i - y_j $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([4, 5])\n",
        "\n",
        "# ‚úèÔ∏è your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Output:\n",
        "\n",
        "```\n",
        "tensor([[-3, -4],\n",
        "        [-2, -3],\n",
        "        [-1, -2]]) <shape: torch.Size([3, 2])> <dtype: torch.int64>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### üìñ **Broadcasting, let's take a peek under the hood**\n",
        "\n",
        "To recap: if a PyTorch operation supports broadcast, then **its tensor arguments can be implicitly expanded to be of equal sizes** (without making copies of the data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### **Broadcastable tensors**\n",
        "\n",
        "Two tensors are \"broadcastable\" if:\n",
        "- Each tensor has at least one dimension\n",
        "- When iterating over the dimension sizes, starting at the trailing dimension, the dimension **sizes** must either **be equal**, **one of them is 1**, or **one of them does not exist**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### **Broadcasting rules**\n",
        "\n",
        "Broadcasting two tensors together follows these rules:\n",
        "\n",
        "1. If the input tensors have different ranks, **singleton dimensions are prepended to the shape** of the smaller one until it has the same rank as the other\n",
        "2. The size in each dimension of the **output shape** is the maximum size in that dimension between the two tensors\n",
        "3. An input can be used in the computation if its size in a particular **dimension either matches** the output size in that dimension, **or is a singleton dimension**\n",
        "4. If an input has a dimension size of 1 in its shape, the **first data entry in that dimension will be used for all calculations** along that dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example**:\n",
        "\n",
        "- `m` has shape `[4, 3]`\n",
        "- `v` has shape `[3,]`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_arr(m, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = m + v\n",
        "print_arr(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Following the Broadcasting logic, this is what happened:\n",
        "\n",
        "- `v` has less dims than `m` so a dimension of `1` is **prepended** $\\to$ `v` is now `[1, 3]`.\n",
        "- Output shape will be `[max(1, 4), max(3, 3)] = [4, 3]`.\n",
        "- `dim 1` of `v` matches exactly `3`; `dim 0` is `1`, so we can use the first data entry in that dimension (i.e. the whole `row 0` of `v`) each time any row is accessed. This is effectively like converting `v` from `[1, 3]` to `[4, 3]` by stacking the repeated row four times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "For more on broadcasting, see the [documentation](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n",
        "\n",
        "Functions that support broadcasting are known as universal functions (i.e. ufuncs). For Numpy you can find the list of all universal functions in the [documentation](https://docs.scipy.org/doc/numpy/reference/ufuncs.html#available-ufuncs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **EXERCISE (2 Point)**\n",
        ">\n",
        "> Given a tensor $Y \\in \\mathbb{R}^{n \\times m}$ and an index pair $(a,b)$, for each element of $Y$ compute its $L_p$ distance to $(a,b)$, and store the resulting distance value in the corresponding cell of $Y$.\n",
        ">\n",
        "> In brief, compute:\n",
        "> $$ y_{ij} = d_{L_p}\\left( (i,j), (a,b) \\right) \\text{ for all }  i,j$$\n",
        ">\n",
        "> and visualize the resulting $Y$.\n",
        ">\n",
        "> Try different values of $p>0$ to see what happens.\n",
        ">\n",
        "> ---\n",
        ">\n",
        "> The [$L_p$ distance](https://en.wikipedia.org/wiki/Lp_space#The_p-norm_in_finite_dimensions) between two points $x$ and $y$ can be computed as: $d_{L_p}(x, y)=\\left( \\sum_{i=1}^n|x_i - y_i|^p\\right)^{1/p}$\n",
        ">\n",
        "> Example: The $L_1$ distance between $(i,j) = (3, 5)$ and $(a,b) = (14, 20)$ is:\n",
        "> $$ y_{3,5} = d_{L_1}( (3, 5), (14, 20) ) = |3 - 14| + |5 - 20| $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Utility function, you can execute and safely ignore this cell\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "def plot_row_images(images: Union[torch.Tensor, np.ndarray]) -> None:\n",
        "  \"\"\" Plots the images in a subplot with multiple rows.\n",
        "\n",
        "  Handles correctly grayscale images.\n",
        "\n",
        "  :param images: tensor with shape [number of images, width, height, <colors>]\n",
        "  \"\"\"\n",
        "  from plotly.subplots import make_subplots\n",
        "  import plotly.graph_objects as go\n",
        "  fig = make_subplots(rows=1, cols=images.shape[0] ,\n",
        "                      specs=[[{}] * images.shape[0]])\n",
        "\n",
        "  # Convert grayscale image to something that go.Image likes\n",
        "  if images.dim() == 3:\n",
        "    images = torch.stack((images, images, images), dim= -1)\n",
        "  elif (images.dim() == 4 and images.shape[-1] == 1):\n",
        "    images = torch.cat((images, images, images), dim= -1)\n",
        "\n",
        "  assert images.shape[-1] == 3 or images.shape[-1] == 4\n",
        "\n",
        "  for i in range(images.shape[0]):\n",
        "    i_image = np.asarray(images[i, ...])\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Image(z = i_image, zmin=[0, 0, 0, 0], zmax=[1, 1, 1, 1]),\n",
        "        row=1, col=i + 1\n",
        "    )\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "# When using plotly pay attention that it often does not like PyTorch Tensors\n",
        "# ...and it does not give any error, just a empty plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.zeros(300, 300)\n",
        "a = 150\n",
        "b = 150\n",
        "\n",
        "x[a, b] = 1  # this will be overwritten by your distance-calculating code\n",
        "plot_row_images(x[None, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚úèÔ∏è your code here\n",
        "\n",
        "# First write a function to calculate the distance.\n",
        "# Fill the matrix with the Lp distance of each point to the center.\n",
        "def distance(input: torch.Tensor, p: int) -> torch.Tensor:\n",
        "    raise NotImplementedError(\"You need to implement this function\")\n",
        "\n",
        "\n",
        "P = 1\n",
        "# Call you function and plot ...\n",
        "mat = distance(x, P)\n",
        "px.imshow(mat).show()\n",
        "P = 8\n",
        "# Call you function and plot ...\n",
        "mat = distance(x, P)\n",
        "px.imshow(mat).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Output for P=1:\n",
        "\n",
        "![L1 expected output](../data/l1_norm_expected_plot.png)\n",
        "\n",
        "Expected Output for P=8:\n",
        "\n",
        "![L8 expected output](../data/l8_norm_expected_plot.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Non-elementwise operations**\n",
        "\n",
        "\n",
        "PyTorch and NumPy provide many useful functions to perform computations on tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2, 3], [3, 4, 5]], dtype=torch.float32)\n",
        "print_arr(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sum up all the elements\n",
        "print_arr(torch.sum(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the mean of each column\n",
        "print_arr(torch.mean(x, dim=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **REMEMBER!**\n",
        ">\n",
        "> In order to avoid confusion with the `dim` parameter, you can think of it as an **index over the list returned by `tensor.shape`**. The operation is performed by iterating over that dimension.\n",
        ">\n",
        "> Example above: since our tensor `x` has shape `[2, 3]`, the dimension `dim=0` operates along the `2`.\n",
        ">\n",
        "> Visually (here array means _tensor_):\n",
        ">\n",
        "><img src=\"https://qph.fs.quoracdn.net/main-qimg-30be20ab9458b5865b526d287b4fef9a\" width=\"500\" >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_arr(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the product of each row\n",
        "print_arr(torch.prod(x, dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Max along the rows (i.e. max value in each column)\n",
        "values, indices = torch.max(x, dim=0)\n",
        "print_arr(values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Max along the columns (i.e. max value in each row)\n",
        "values, indices = torch.max(x, dim=1)\n",
        "print_arr(values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Dim parameter, let's take a peek under the hood**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see what the `dim` parameter exactly does:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dim = 2\n",
        "\n",
        "a = torch.arange(2*3*4).reshape(2, 3, 4)\n",
        "out = a.sum(dim=dim)\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# It is summing over the `dim` dimension, i.e.:\n",
        "a.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The `dim` dimension has 4 elements\n",
        "a.shape[dim]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The dimension dim collapses, the output tensor will have shape:\n",
        "new_shape = a.shape[:dim] + a.shape[dim + 1:]\n",
        "new_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explicitly compute the sum over dim\n",
        "out = torch.zeros(new_shape)\n",
        "\n",
        "# iterate over all the rows\n",
        "for r in range(a.shape[0]):\n",
        "  # iterate over all the columns in the r-th row\n",
        "  for c in range(a.shape[1]):\n",
        "\n",
        "    for i in range(a.shape[dim]): # <- sum over 'dim'\n",
        "\n",
        "      out[r, c] += a[r, c, i]\n",
        "\n",
        "out\n",
        "\n",
        "# **DO NOT** use for loops in your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### **EXERCISE**\n",
        ">\n",
        "> Given a matrix $X \\in R^{k \\times k}$:\n",
        "> - Compute the mean of the values along its diagonal.\n",
        ">\n",
        "> Perform this computation in at least two different ways, then check that the results are the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.rand(4, 4)\n",
        "print_arr(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚úèÔ∏è your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With input x = \n",
        "\n",
        "```\n",
        "[[0.0259, 0.9557, 0.8247, 0.2847],\n",
        "[0.6301, 0.8209, 0.2687, 0.1967],\n",
        "[0.1685, 0.0114, 0.9896, 0.5087],\n",
        "[0.4334, 0.5446, 0.1283, 0.8798]]\n",
        "```\n",
        "\n",
        "The expected output is:\n",
        "\n",
        "```\n",
        "tensor(0.6791) <shape: torch.Size([])> <dtype: torch.float32>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **EXERCISE**\n",
        ">\n",
        "> Given a binary non-symmetric matrix $X \\in \\{0, 1\\}^{n\\times n}$, build the symmetric matrix $Y \\in \\{0, 1\\}^{n \\times n}$ defined as:\n",
        "> $$\n",
        "y_{ij} =\n",
        "\\begin{cases}\n",
        "1 & \\text{if } x_{ij} = 1 \\\\\n",
        "1 & \\text{if } x_{ji} = 1 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        ">\n",
        "> *Hint*: search for `clamp` in the [docs](https://pytorch.org/docs/stable/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.randint(0, 2, (5, 5))  # Non-symmetric matrix\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚úèÔ∏è your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With input x = \n",
        "\n",
        "```\n",
        "tensor([[0, 1, 0, 1, 1],\n",
        "        [0, 0, 1, 0, 0],\n",
        "        [1, 0, 1, 1, 0],\n",
        "        [0, 1, 0, 0, 1],\n",
        "        [1, 1, 0, 0, 1]])\n",
        "```\n",
        "\n",
        "the expected output is:\n",
        "\n",
        "```\n",
        "tensor([[0, 1, 1, 1, 1],\n",
        "        [1, 0, 1, 1, 1],\n",
        "        [1, 1, 1, 1, 0],\n",
        "        [1, 1, 1, 0, 1],\n",
        "        [1, 1, 0, 1, 1]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Tensor contractions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Matrix multiplication**\n",
        "\n",
        "Given $X \\in R^{n \\times d}$ and $Y \\in R^{d \\times v}$, their matrix multiplication $Z \\in R^{n \\times v}$ is defined as:\n",
        "\n",
        "$$ \\sum_{k=0}^{d} x_{ik} y_{kj} = z_{ij} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "y = torch.tensor([[1, 2], [2, 1]])\n",
        "print_arr(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# as we will see, matmul's functionality is not limited to matrix-matrix multiplication\n",
        "torch.matmul(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x @ y  # Operator overload for matmul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.mm(x, y)  # PyTorch function, only works for rank-2 tensors (matrices) https://pytorch.org/docs/stable/generated/torch.mm.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x.mm(y)  # Tensor method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.einsum('ik, kj -> ij', (x, y))  # Einsum notation!\n",
        "\n",
        "# It summed up dimension labeled with the index `k`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Dot product**\n",
        "Also known as scalar product or inner product.\n",
        "Given $x \\in \\mathbb{R}^k$ and $y \\in \\mathbb{R}^k$, the dot product $z \\in \\mathbb{R}$ is defined as:\n",
        "\n",
        "$$ \\sum_{i=0}^{k} x_i y_i = z $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([4, 5, 6])\n",
        "print_arr(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We want to perform:\n",
        "(1 * 4) + (2 * 5) + (3 * 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.dot(x, y)  # PyTorch function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x.dot(y) # Tensor method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x @ y  # PyTorch operator again overloading matmul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.einsum('i, i ->', (x, y))  # Einstein notation!\n",
        "\n",
        "# Read it as:\n",
        "# - iterate with i along x\n",
        "# - iterate with i along y\n",
        "# - compute the product at each iteration\n",
        "# - sum the products and return a scalar (-> means return a scalar)\n",
        "\n",
        "# More in general, Einstein notation:\n",
        "# Multiply point-wise repeated indices in the input\n",
        "# Sum up along the indices that `do not` appear in the output\n",
        "\n",
        "# More on this below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Batch matrix multiplication**\n",
        "\n",
        "Often we want to perform more operations together. Why?\n",
        "- Reduce the **overhead of uploading** each tensor to/from the GPU memory\n",
        "- **Better parallelization** of the computation\n",
        "\n",
        "Given two 3D tensors, each one containing ``b`` matrices,\n",
        "$X \\in \\mathbb{R}^{b \\times n \\times m}$\n",
        "and  \n",
        "$Y \\in \\mathbb{R}^{b \\times m \\times p}$,\n",
        "\n",
        "We want to multiply together each $i$-th pair of matrices, obtaining a tensor $Z \\in \\mathbb{R}^{b \\times n \\times p}$ defined as:\n",
        "\n",
        "$$ z_{bij} = \\sum_{k=0}^m x_{bik} y_{bkj} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# here b = 2 matrices\n",
        "x = torch.tensor([[[1, 2], [3, 4], [5, 6]], [[1, 2], [3, 4], [5, 6]]])  # 3x2 matrices\n",
        "y = torch.tensor([[[1, 2], [2, 1]], [[1, 2], [2, 1]]])  # 2x2 matrices\n",
        "print_arr(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.bmm(x, y)  # **not** torch.mm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Operator overload! again, matmul is actually doing the job\n",
        "x @ y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.einsum('bik, bkj -> bij', (x, y)) # Einstein notation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Broadcast matrix multiplication**\n",
        "\n",
        "Given a matrix $Y \\in \\mathbb{R}^{m \\times p}$ and $b$ matrices of size $n \\times m$ organized in a 3D tensor $X \\in \\mathbb{R}^{b \\times n \\times m}$, we want to multiply together each matrix $X_{i,:,:}$ with $Y$, obtaining a tensor $Z \\in R^{b \\times n \\times p}$ defined as:\n",
        "\n",
        "$$ z_{bij} = \\sum_{k=0}^m x_{bik} y_{kj} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.tensor([[[1, 2], [3, 4], [5, 6]], [[1, 2], [3, 4], [5, 6]]])\n",
        "y = torch.tensor([[1, 2], [2, 1]])\n",
        "print_arr(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.matmul(x, y)  # always uses the last two dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x @ y   # still using the last two dimensions since @ overloads matmul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **EXERCISE**\n",
        ">\n",
        "> Use the einsum notation to compute the equivalent broadcast matrix multiplication!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Yor code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Output:\n",
        "\n",
        "```\n",
        "tensor([[[ 5,  4],\n",
        "         [11, 10],\n",
        "         [17, 16]],\n",
        "\n",
        "        [[ 5,  4],\n",
        "         [11, 10],\n",
        "         [17, 16]]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Einsum notation**\n",
        "\n",
        "Einstein notation is a way to express complex operations on tensors.\n",
        "\n",
        "- It is **concise but expressive enough** to perform almost every operation you will need in building your neural networks, allowing you to think of the only thing that matters... **dimensions!**\n",
        "- You will **not need to check your dimensions** after an einsum operation, since the dimensions themselves are *defining* the tensor operation.\n",
        "- You will **not need to shape-comment** your tensors. Those comments do not work: they are bound to get outdated.\n",
        "-  You will not need to explicitly code **intermediate operations** such as reshaping, transposing and intermediate tensors.\n",
        "- It is **not library-specific**, being avaiable in ``numpy``, ``pytorch``, ``tensorflow`` and ``jax`` with the same signature. So you do not need to remember the functions signature in all the frameworks.\n",
        "- It can sometimes be compiled to high-performing code (e.g. [Tensor Comprehensions](https://pytorch.org/blog/tensor-comprehensions/))\n",
        "\n",
        "Check [this blog post by Olexa Bilaniuk](https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/) to take a peek under the hood of einsum and [this one by Tim Rockt√§schel](https://rockt.github.io/2018/04/30/einsum) for several examples.\n",
        "\n",
        "Its formal behavior is well described in the [Numpy documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html).\n",
        "However, it is very intuitive and better explained through examples.\n",
        "\n",
        "![alt text](https://obilaniu6266h16.files.wordpress.com/2016/02/einsum-fmtstring.png?w=676)\n",
        "\n",
        "> *Historical note (taken from O.Bilaniuk's post)*\n",
        ">\n",
        "> Einstein had no part in the development of this notation. He merely popularized it, by expressing his entire theory of General Relativity in it. In a letter to [Tullio Levi-Civita](https://en.wikipedia.org/wiki/Tullio_Levi-Civita), co-developer alongside [Gregorio Ricci-Curbastro](https://en.wikipedia.org/wiki/Gregorio_Ricci-Curbastro) of Ricci calculus (of which this summation notation was only a part), Einstein wrote:\n",
        ">\n",
        "> \" *I admire the elegance of your method of computation; it must be nice to ride through these fields upon the horse of true mathematics while the like of us have to make our way laboriously on foot.* \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.arange(6).reshape(2, 3)  # will use this in the examples below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### **Matrix transpose**\n",
        "\n",
        "$$ B_{ji} = A_{ij} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The characters are indices along each dimension\n",
        "b = torch.einsum('ij -> ji', a)\n",
        "print_arr(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### **Sum**\n",
        "\n",
        "$$ b = \\sum_i \\sum_j A_{ij} := A_{ij} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Indices that do not appear in the output tensor are summed up\n",
        "b = torch.einsum('ij -> ', a)\n",
        "print_arr(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### **Column sum**\n",
        "\n",
        "$$ b_j = \\sum_i A_{ij} := A_{ij} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Indices that do not appear in the output tensor are summed up,\n",
        "# ...even if some other index appears\n",
        "b = torch.einsum('ij -> j', a)\n",
        "print_arr(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### **EXERCISE**\n",
        ">\n",
        "> Given a binary tensor $X \\in \\{0, 1\\}^{n \\times m}$ return a tensor $y \\in \\mathbb{R}^{n}$ that has in the $i$-th position the **number of ones** in the $i$-th row of $X$.\n",
        ">\n",
        ">Give a solution using `einsum`, and a solution using standard manipulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = (torch.rand(100, 200) > 0.5).int()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display a binary matrix with plotly\n",
        "\n",
        "fig = px.imshow(x)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "row_ones_einsum = ...  # your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check your result by comparing your result with the sum operator\n",
        "\n",
        "row_ones = torch.sum(x, dim=-1)  # recall that -1 refers to the last dimension\n",
        "\n",
        "torch.equal(row_ones_einsum, row_ones) # True if the two tensors are equal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "px.imshow(row_ones[:, None]).show()\n",
        "print(f'Sum up the row counts: {row_ones.sum()}\\nSum directly all the ones in the matrix: {x.sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### **Matrix-vector multiplication**\n",
        "\n",
        "$$ c_i = \\sum_k A_{ik}b_k := A_{ik}b_k $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Repeated indices in different input tensors indicate pointwise multiplication\n",
        "a = torch.arange(6).reshape(2, 3)\n",
        "b = torch.arange(3)\n",
        "c = torch.einsum('ik, k -> i', [a, b])  # Multiply on k, then sum up on k\n",
        "print_arr(a, b, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### **Matrix-matrix multiplication**\n",
        "\n",
        "$$ C_{ij} = \\sum_k A_{ik}B_{kj} := A_{ik}B_{kj} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üìñ Understanding einsum, what happens inside?\n",
        "\n",
        "![alt text](https://obilaniu6266h16.files.wordpress.com/2016/02/einsum-matrixmul.png?w=676)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.arange(6).reshape(2, 3)\n",
        "b = torch.arange(15).reshape(3, 5)\n",
        "c = torch.einsum('ik, kj -> ij', [a, b])\n",
        "print_arr(a, b, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### **Dot product multiplication**\n",
        "\n",
        "$$ c = \\sum_i a_i b_i := a_i b_i $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.arange(3)\n",
        "b = torch.arange(3,6)\n",
        "c = torch.einsum('i,i->', (a, b))\n",
        "print_arr(a, b, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### **Point-wise multiplication**\n",
        "Also known as Hadamard product:\n",
        "\n",
        "$$ C_{ij} = A_{ij} B_{ij} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.arange(6).reshape(2, 3)\n",
        "b = torch.arange(6,12).reshape(2, 3)\n",
        "c = torch.einsum('ij, ij -> ij', (a, b))\n",
        "print_arr(a, b, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### **Outer product**\n",
        "Given two column vectors of length $m$ and $n$ respectively,\n",
        "\\begin{align*}\n",
        "\\mathbf{a}=\\left[\\begin{array}{c}\n",
        "a_{1} &\n",
        "a_{2} &\n",
        "\\dots &\n",
        "a_{m}\n",
        "\\end{array}\\right]^\\top, \\quad \\mathbf{b}=\\left[\\begin{array}{c}\n",
        "b_{1} &\n",
        "b_{2} &\n",
        "\\dots &\n",
        "b_{n}\n",
        "\\end{array}\\right]^\\top\n",
        "\\end{align*}\n",
        "their outer product, denoted $\\mathbf{a} \\otimes \\mathbf{b}$, is defined as the $m \\times n$ matrix $\\mathbf{C}$ obtained by multiplying each element of $\\mathbf{a}$ by each element of $\\mathbf{b}$:\n",
        "\\begin{align*}\n",
        "\\mathbf{a} \\otimes \\mathbf{b}=\\mathbf{C}=\\left[\\begin{array}{cccc}\n",
        "a_{1} b_{1} & a_{1} b_{2} & \\ldots & a_{1} b_{n} \\\\\n",
        "a_{2} b_{1} & a_{2} b_{2} & \\ldots & a_{2} b_{n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{m} b_{1} & a_{m} b_{2} & \\ldots & a_{m} b_{n}\n",
        "\\end{array}\\right]\n",
        "\\end{align*}\n",
        "Or, in index notation,\n",
        "$$ C_{ij} = a_i b_j $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.arange(3)\n",
        "b = torch.arange(3,7)\n",
        "c = torch.einsum('i, j -> ij', (a, b))\n",
        "print_arr(a, b, c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using the standard PyTorch API\n",
        "torch.outer(a, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using broadcasting black magic\n",
        "a[:, None] * b[None, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### üìñ **Batch matrix multiplication**\n",
        "\n",
        "$$ c_{bij} = \\sum_k a_{bik} b_{bkj} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.randn(2,2,5)\n",
        "b = torch.randn(2,5,3)\n",
        "c = torch.einsum('bik,bkj->bij', [a, b])\n",
        "print_arr(a, b, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Singleton dimensions\n",
        "\n",
        " In deep learning it is very common to **add or remove dimensions of size $1$** in a tensor. As we mentioned, this is called **unsqueezing** and **squeezing**, and it occurs often during batch processing, manipulating feature maps, making network layers compatible, and in several other occasions.\n",
        "\n",
        " It is possible to perform these operations in different ways, feel free to use\n",
        " whatever is more comfortable to you! Again, **prefer readability to cryptic one-liners** for the sanity of a hypothetical unknown reader or your future self.\n",
        "\n",
        "In the example below, we transform a rank-1 tensor into a rank-2 \"column\", and back to a rank-1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a rank-1 tensor we will use\n",
        "x = torch.arange(6)\n",
        "print_arr(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transform **`x` into a column tensor** in four different ways.\n",
        "\n",
        "Remember that the shape of a column tensor is in the form: `(rows, 1)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1)\n",
        "# Use the `reshape` or `view` functions\n",
        "\n",
        "y1 = x.reshape(-1, 1)\n",
        "y2 = x.view(-1, 1)\n",
        "\n",
        "print_arr(y1, y2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2)\n",
        "# Use the specific `unsqueeze` function to unsqueeze a dimension\n",
        "\n",
        "y3 = x.unsqueeze(dim=-1)\n",
        "y4 = x.unsqueeze(dim=1)\n",
        "\n",
        "print_arr(y3, y4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3)\n",
        "# Explicitly index a non-existing dimension with `None`\n",
        "\n",
        "y5 = x[:, None]\n",
        "\n",
        "print_arr(y5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4)\n",
        "# Same as before, but do not assume a rank-2 tensor and index the last one.\n",
        "# This approach is useful to write functions that work both for\n",
        "# batched or non-batched data\n",
        "\n",
        "y6 = x[..., None]\n",
        "\n",
        "print_arr(y5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we go back to a rank-1 tensor\n",
        "\n",
        "x1 = y1.reshape(-1)\n",
        "x2 = y2.view(-1)          # Explicity enforce to get a view of the tensors, without copying data\n",
        "x3 = y3.squeeze(dim=-1)\n",
        "x4 = y4.squeeze(dim=1)\n",
        "x5 = y5[:, 0]             # Manually collapse the dimension with an integer indexing\n",
        "x6 = y6[..., 0]\n",
        "\n",
        "print_arr(x1, x2, x3, x4, x5, x6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NOTE**\n",
        ">\n",
        "> indexing with `...` means  **keeping all the other dimensions the same**.\n",
        "> Keep in mind that `...` is just a Python singleton object (just as `None`).\n",
        "> Its type is Ellipsis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.rand(3,3,3)\n",
        "x[:, :, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x[..., 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tensor types\n",
        "Pay attention to the tensor types!\n",
        "Several methods are available to convert tensors to different types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.rand(3, 3) + 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a.int()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a.long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a.double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a.bool()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a.to(torch.double)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a.to(torch.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a.bool().int()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pro tip:** Do not try to memorize all the PyTorch API!\n",
        "\n",
        "> Learn to understand what operation should already exist and search for it, when you need it. If it is something common, and it usually is, chances are it already exists.\n",
        "\n",
        "Google, StackOverflow and the docs are your friends!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Einops\n",
        "\n",
        "If you liked the `einsum` operation, have fun with the [einops](https://github.com/arogozhnikov/einops) package! üöÄ\n",
        "\n",
        "It is a third-party library, compatible with most frameworks, that brings superpowers to `einsum`. We will not use the `einops` library in the tutorials, however, feel free to read the [docs](https://github.com/arogozhnikov/einops) and use it.\n",
        "\n",
        "![](http://arogozhnikov.github.io/images/einops/einops_video.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These final exercises are designed to showcase the elegant solutions of einsum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### **EXERCISE 1 (2 Points)**\n",
        ">\n",
        "> You are given $b$ images with size $w \\times h$. Each pixel in each image has three color channels, `(r, g, b)`. These images are organized in a tensor $X \\in \\mathbb{R}^{w \\times b \\times c \\times h}$.\n",
        ">\n",
        "> You want to apply a linear trasformation to the color channel of each single image. In particular, you want to :\n",
        "> - **Convert each image into a grey scale image**.\n",
        "> - **Afterthat, transpose the images** to swap the height and width.\n",
        ">\n",
        "> The linear traformation that converts from `(r, g, b)` to grey scale is simply a linear combination of `r`, `g` and `b`. It can be encoded in the following 1-rank tensor $y \\in \\mathbb{R}^3$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = torch.tensor([0.2989, 0.5870, 0.1140], dtype=torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "> At the end, you want to obtain a tensor $Z \\in \\mathbb{R}^{b \\times w \\times h}$.\n",
        ">\n",
        "> Write the PyTorch code that performs this operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the input tensors for the exercise\n",
        "# Execute and ignore this cell\n",
        "\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "\n",
        "size = 100\n",
        "\n",
        "image1 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Earth_Eastern_Hemisphere.jpg/260px-Earth_Eastern_Hemisphere.jpg')\n",
        "image1 = torch.from_numpy(resize(image1, (size, size), anti_aliasing=True)).float()  # Covert  to float type\n",
        "image1 = image1[..., :3]  # remove alpha channel\n",
        "\n",
        "image2 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/The_Sun_by_the_Atmospheric_Imaging_Assembly_of_NASA%27s_Solar_Dynamics_Observatory_-_20100819.jpg/628px-The_Sun_by_the_Atmospheric_Imaging_Assembly_of_NASA%27s_Solar_Dynamics_Observatory_-_20100819.jpg')\n",
        "image2 = torch.from_numpy(resize(image2, (size, size), anti_aliasing=True)).float()\n",
        "image2 = image2[..., :3]  # remove alpha channel\n",
        "\n",
        "image3 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/8/80/Wikipedia-logo-v2.svg/1920px-Wikipedia-logo-v2.svg.png')\n",
        "image3 = torch.from_numpy(resize(image3, (size, size), anti_aliasing=True)).float()\n",
        "image3 = image3[..., :3]  # remove alpha channel\n",
        "\n",
        "source_images = torch.stack((image1, image2, image3), dim=0)\n",
        "images = torch.einsum('bwhc -> wbch', source_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot source images\n",
        "plot_row_images(source_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚úèÔ∏è your code here\n",
        "gray_images = ...\n",
        "\n",
        "gray_images_tr = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the gray images\n",
        "plot_row_images(gray_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Output:\n",
        "\n",
        "![grayscale expected output](../data/grayscale.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the gray transposed images\n",
        "plot_row_images(gray_images_tr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Output:\n",
        "\n",
        "![transposed expected output](../data/transposed.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **EXERCISE 2**\n",
        ">\n",
        "> Given $k$ points organized in a tensor $X \\in \\mathbb{R}^{k \\times 2}$ apply a reflection along the $y$ axis as a linear transformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define some points in R^2\n",
        "x = torch.arange(100, dtype=torch.float)\n",
        "y = x ** 2\n",
        "\n",
        "# Define some points in R^2\n",
        "data = torch.stack((x, y), dim=0).t()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "px.scatter(x = data[:, 0].numpy(), y = data[:, 1].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚úèÔ∏è your code here\n",
        "mirrored_data = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the new points\n",
        "px.scatter(x = mirrored_data[:, 0].numpy(), y = mirrored_data[:, 1].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Output:\n",
        "\n",
        "![inverted expected output](../data/inverted.pngnewplot.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **EXERCISE 3**\n",
        ">\n",
        ">  You are given $b$ images with size $w \\times h$. Each pixel in each image has `(r, g, b)` channels. These images are organized in a tensor $X \\in \\mathbb{R}^{w \\times b \\times c \\times h}$, i.e. the same tensor as in the exercise 1.\n",
        ">\n",
        "> You want to swap the `red` color with the `blue` color, and decrease the intensity of the `green` by half.\n",
        ">\n",
        "> Perform the transformation on all the images simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚úèÔ∏è your code here\n",
        "rb_images = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expected Output:\n",
        "\n",
        "![transposed expected output](../data/change_color.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_row_images(rb_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **EXERCISE 4 (4 Point)**\n",
        ">\n",
        ">  You are given $b$ images with size $w \\times h$. Each pixel in each image has `(r, g, b)` colors. These images are organized in a tensor $X \\in \\mathbb{R}^{w \\times b \\times c \\times h}$, i.e. the same tensor as exercise 1 and 3.\n",
        ">\n",
        "> You want to **convert each image into a 3D point cloud**:\n",
        "> - the `(x, y)` coordinates of each point in the point cloud are the **indices** of the pixels in the original image\n",
        "> - the `z` coordinate of each point in the point cloud is the $L_2$ norm of the color of the corresponding pixel, multiplied by $10$\n",
        ">\n",
        "> *Hint*: you may need some other PyTorch function, search the docs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill the missing code\n",
        "\n",
        "# Just normalize the tensor into the common form [batch, width, height, colors]\n",
        "imgs = # Fill here\n",
        "print(imgs.shape) # 3 images, 100x100 pixels, 3 colors\n",
        "\n",
        "# The x, y coordinate of the point cloud are all the possible pairs of indices (i, j)\n",
        "row_indices = torch.arange(imgs.shape[1], dtype=torch.float)\n",
        "col_indices = torch.arange(imgs.shape[2], dtype=torch.float)\n",
        "xy =  # Fill here\n",
        "# hint: you need the *cartesian product* of the two. Check the PyTorch documentation for a function that does this\n",
        "\n",
        "# Compute the L2 norm for each pixel in each image\n",
        "depth =  # Fill here\n",
        "\n",
        "# For every pair (i, j), retrieve the L2 norm of that pixel\n",
        "z = depth[:, xy[:, 0].long(), xy[:, 1].long()] * 10\n",
        "\n",
        "# Adjust the dimensions, repeat and concatenate accordingly\n",
        "xy = xy.repeat(imgs.shape[0], 1, 1)  # x,y coordinates are constant for the three images\n",
        "# concatenate xy and z\n",
        "clouds =  # Fill here\n",
        "\n",
        "# Three images, 10000 points, each point with coordinates x,y,z in 3D\n",
        "print(clouds.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility function\n",
        "# Execute and ignore this cell\n",
        "\n",
        "from typing import Union\n",
        "\n",
        "def plot_3d_point_cloud(cloud: Union[torch.Tensor, np.ndarray]) -> None:\n",
        "  \"\"\" Plot a single 3D point cloud\n",
        "\n",
        "  :param cloud: tensor with shape [number of points, coordinates]\n",
        "  \"\"\"\n",
        "  import pandas as pd\n",
        "  df = pd.DataFrame(np.asarray(cloud), columns=['x', 'y', 'z'])\n",
        "  fig = px.scatter_3d(df, x=df.x, y=df.y, z=df.z, color=df.z, opacity=1, range_z=[0, 30])\n",
        "  fig.update_layout({'scene_aspectmode': 'data', 'scene_camera':  dict(\n",
        "          up=dict(x=0., y=0., z=0.),\n",
        "          eye=dict(x=0., y=0., z=3.)\n",
        "      )})\n",
        "  fig.update_traces(marker=dict(size=3,),\n",
        "                    selector=dict(mode='markers'))\n",
        "  _ = fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading expected results. Ignpore this cell\n",
        "clouds_gt = np.load(\"../data/clouds.npy\", allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_3d_point_cloud(clouds[0, ...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expected result\n",
        "plot_3d_point_cloud(clouds_gt[0, ...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_3d_point_cloud(clouds[1, ...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expected result\n",
        "plot_3d_point_cloud(clouds_gt[1, ...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_3d_point_cloud(clouds[2, ...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expected result\n",
        "plot_3d_point_cloud(clouds_gt[2, ...])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "i51GvUnwSc0U"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "3Dvision-ex1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
